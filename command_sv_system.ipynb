{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes\n",
    "command: disjoint speaker split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dir = '/home/muncok/DL/dataset/SV_sets/dataframes/'\n",
    "data_dir = '/home/muncok/DL/dataset/SV_sets/speech_commands/'\n",
    "command_df = pd.read_pickle(os.path.join(dataframe_dir,'Command_Dataframe.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: correct splits\n",
    "all_spks = command_df.spk.unique()\n",
    "all_sents = command_df.sent.unique()\n",
    "uttrs_counts = command_df.spk.value_counts()\n",
    "sv_spks = list(uttrs_counts[:10].index)  # top 5 speakers whose number of recored uttrs is high\n",
    "si_df = command_df[~command_df.spk.isin(sv_spks)]\n",
    "si_spks = list(si_df.spk.unique())\n",
    "sv_df = command_df[command_df.spk.isin(sv_spks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spk] train:50034, val:6563, test:6718\n",
      "[sent] train:41423, val:10372, test:11520\n",
      "[random] train:50652, val:6332, test:6331\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# command dataset is disjoint along speakers.\n",
    "si_spk_train = si_df[si_df.set == 'train']\n",
    "si_spk_val = si_df[si_df.set == 'val']\n",
    "si_spk_test = si_df[si_df.set == 'test']\n",
    "print(\"[spk] train:{}, val:{}, test:{}\".format(len(si_spk_train), len(si_spk_val), len(si_spk_test)))\n",
    "\n",
    "# spliting along the sents\n",
    "random.shuffle(all_sents)\n",
    "train_sent = all_sents[:20]\n",
    "val_sent = all_sents[20:25]\n",
    "test_sent = all_sents[25:]\n",
    "si_sent_train = si_df[si_df.sent.isin(train_sent)]\n",
    "si_sent_val = si_df[si_df.sent.isin(val_sent)]\n",
    "si_sent_test = si_df[si_df.sent.isin(test_sent)]\n",
    "print(\"[sent] train:{}, val:{}, test:{}\".format(len(si_sent_train), len(si_sent_val), len(si_sent_test)))\n",
    "\n",
    "# random sampling\n",
    "si_random_train = si_df.sample(frac=0.8)\n",
    "si_random_test = si_df.drop(index=si_random_train.index)\n",
    "si_random_val = si_random_test.sample(frac=0.5)\n",
    "si_random_test = si_random_test.drop(index=si_random_val.index)\n",
    "print(\"[random] train:{}, val:{}, test:{}\".format(len(si_random_train), len(si_random_val), len(si_random_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifests/commands/si_command_train_manifest.csv was written\n",
      "manifests/commands/si_command_val_manifest.csv was written\n",
      "manifests/commands/si_command_test_manifest.csv was written\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "tags = ['train', 'val', 'test']\n",
    "# choose a si split\n",
    "sets = {'train':si_random_train, 'val':si_random_train, 'test':si_random_train}\n",
    "manifest_dir = \"manifests/commands/\"\n",
    "\n",
    "data_dir = \"/home/muncok/DL/dataset/SV_sets/speech_commands_vad/\"\n",
    "for tag in tags:\n",
    "    samples = []\n",
    "    save_path = os.path.join(manifest_dir,'si_{}_{}_manifest.csv'.format(\"command\", tag))\n",
    "    with open(save_path, 'w') as f:\n",
    "        for index, row in sets[tag].iterrows():\n",
    "            file_path = os.path.join(data_dir, row.sent, row.file)\n",
    "            label = row.label\n",
    "            sample = ','.join([file_path, str(label)])\n",
    "            samples.append(sample)\n",
    "        random.shuffle(samples)\n",
    "        writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "        writer.writerow(samples)\n",
    "        print(\"{} was written\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import honk_sv.train as hk\n",
    "from honk_sv import model as mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"cnn-trad-pool2\"\n",
    "dataset = \"command\"\n",
    "\n",
    "global_config = dict(model=model, dataset=dataset,\n",
    "                     no_cuda=False,  gpu_no=0, \n",
    "                     n_epochs=30, batch_size=32,\n",
    "                     lr=[0.001], schedule=[np.inf], dev_every=1, seed=0, use_nesterov=False,\n",
    "                     cache_size=32768, momentum=0.9, weight_decay=0.00001, \n",
    "                     num_workers=8, print_step=100,\n",
    "                     bn_size = 256)\n",
    "\n",
    "builder = hk.ConfigBuilder(\n",
    "                mod.find_config(model),\n",
    "                mod.SpeechDataset.default_config(dataset),\n",
    "                global_config)\n",
    "parser = builder.build_argparse()\n",
    "si_config = builder.config_from_argparse(parser)\n",
    "si_config['model_class'] = mod.find_model(model)\n",
    "hk.set_seed(si_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_config['n_labels'] = 2000\n",
    "manifest_dir = \"manifests/commands/\"\n",
    "for tag in ['train', 'val', 'test']:\n",
    "    si_config['{}_manifest'.format(tag)] =  os.path.join(manifest_dir,'si_{}_{}_manifest.csv'.format(\"command\", tag))\n",
    "\n",
    "si_model = si_config['model_class'](si_config)\n",
    "si_config['input_file'] = \"\"\n",
    "si_config['output_file'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "model = \"lstm\"\n",
    "dataset = \"command\"\n",
    "\n",
    "global_config = dict(model=model, dataset=dataset,\n",
    "                     no_cuda=False,  gpu_no=0, \n",
    "                     n_epochs=30, batch_size=32,\n",
    "                     lr=[0.01], schedule=[np.inf], dev_every=1, seed=0, use_nesterov=False,\n",
    "                     cache_size=32768, momentum=0.9, weight_decay=0.00001, \n",
    "                     num_workers=8, print_step=100,\n",
    "                     h_dim=500, n_layer=1)\n",
    "\n",
    "builder = hk.ConfigBuilder(\n",
    "                mod.find_config(model),\n",
    "                mod.SpeechDataset.default_config(dataset),\n",
    "                global_config)\n",
    "parser = builder.build_argparse()\n",
    "si_config = builder.config_from_argparse(parser)\n",
    "si_config['model_class'] = mod.find_model(model)\n",
    "hk.set_seed(si_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_config['n_labels'] = 2000\n",
    "manifest_dir = \"manifests/commands/\"\n",
    "for tag in ['train', 'val', 'test']:\n",
    "    si_config['{}_manifest'.format(tag)] =  os.path.join(manifest_dir,'si_{}_{}_manifest.csv'.format(\"command\", tag))\n",
    "\n",
    "si_config['bn_size'] = 256\n",
    "si_model = si_config['model_class'](si_config)\n",
    "si_config['input_file'] = \"\"\n",
    "si_config['output_file'] = \"models/si_command_lstm_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechModel (\n",
       "  (conv1): Conv2d(1, 64, kernel_size=[20, 8], stride=(1, 1))\n",
       "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=[10, 4], stride=(1, 1))\n",
       "  (pool2): MaxPool2d (size=(1, 1), stride=(1, 1), dilation=(1, 1))\n",
       "  (bottleneck): Linear (26624 -> 256)\n",
       "  (output): Linear (256 -> 2000)\n",
       "  (dropout): Dropout (p = 0.5)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:243",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b23bb8704acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msi_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msi_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, model)\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0mmax_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msv_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/train.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(config, model, test_loader)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mmodel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, feature)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMTLSpeechModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpeechModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch at /pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:243"
     ]
    }
   ],
   "source": [
    "hk.train(si_config, model=si_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e432961f5872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msi_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/command/si_command_random_model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msi_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/train.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(config, model, test_loader)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mmodel_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_cuda\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/model.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/model.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, example, silence)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbg_noise\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;31m# data = torch.from_numpy(fft_audio(data, self.window_size,self.window_stride))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_audio_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DL/projects/sv_system/honk_sv/manage_audio.py\u001b[0m in \u001b[0;36mpreprocess_audio\u001b[0;34m(data, n_mels, dct_filters)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdct_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, power, **kwargs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length,\n\u001b[0;32m-> 1388\u001b[0;31m                             power=power)\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;31m# Build a Mel filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Otherwise, compute a magnitude spectrogram from input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    178\u001b[0m         stft_matrix[:, bl_s:bl_t] = fft.fft(fft_window *\n\u001b[1;32m    179\u001b[0m                                             \u001b[0my_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbl_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbl_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                                             axis=0)[:stft_matrix.shape[0]].conj()\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstft_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/scipy/fftpack/basic.py\u001b[0m in \u001b[0;36mfft\u001b[0;34m(x, n, axis, overwrite_x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwork_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverwrite_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "si_model.load(\"models/command/si_command_random_model.pt\")\n",
    "hk.evaluate(si_config,si_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KWS Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_sents = [\"Yes\", \"No\", \"Up\", \"Down\", \"Left\",\n",
    "             \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\",\n",
    "             \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"] \n",
    "main_sents = list(map(lambda x: x.lower(), main_sents))\n",
    "sent_labels = list(main_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kws split contains only main sentences\n",
    "kws_train = si_df[(si_df.set == 'train') & (si_df.sent.isin(main_sents))]\n",
    "kws_val = si_df[(si_df.set == 'val') & (si_df.sent.isin(main_sents))]\n",
    "kws_test = si_df[(si_df.set == 'test') & (si_df.sent.isin(main_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifests/commands/kws/kws_command_train_manifest.csv was written\n",
      "manifests/commands/kws/kws_command_val_manifest.csv was written\n",
      "manifests/commands/kws/kws_command_test_manifest.csv was written\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "tags = ['train', 'val', 'test']\n",
    "sets = {'train':kws_train, 'val':kws_val, 'test':kws_test}\n",
    "manifest_dir = \"manifests/commands/kws\"\n",
    "\n",
    "for tag in tags:\n",
    "    samples = []\n",
    "    save_path = os.path.join(manifest_dir,'kws_{}_{}_manifest.csv'.format(\"command\", tag))\n",
    "    with open(save_path, 'w') as f:\n",
    "        for index, row in sets[tag].iterrows():\n",
    "            file_path = os.path.join(data_dir, row.sent, row.file)\n",
    "            label = sent_labels.index(row.sent)\n",
    "            sample = ','.join([file_path, str(label)])\n",
    "            samples.append(sample)\n",
    "        random.shuffle(samples)\n",
    "        writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "        writer.writerow(samples)\n",
    "        print(\"{} was written\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"cnn-trad-pool2\"\n",
    "dataset = \"command\"\n",
    "\n",
    "global_config = dict(model=model, dataset=dataset,\n",
    "                     no_cuda=False,  gpu_no=0, \n",
    "                     n_epochs=30, batch_size=32,\n",
    "                     lr=[0.001], schedule=[np.inf], dev_every=1, seed=0, use_nesterov=False,\n",
    "                     cache_size=32768, momentum=0.9, weight_decay=0.00001, \n",
    "                     num_workers=8, print_step=100,\n",
    "                     bn_size = 256, silence_prob = 0)\n",
    "\n",
    "builder = hk.ConfigBuilder(\n",
    "                mod.find_config(model),\n",
    "                mod.SpeechDataset.default_config(dataset),\n",
    "                global_config)\n",
    "parser = builder.build_argparse()\n",
    "kws_config = builder.config_from_argparse(parser)\n",
    "kws_config['model_class'] = mod.find_model(model)\n",
    "hk.set_seed(kws_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_dir = \"manifests/commands/kws\"\n",
    "kws_config['n_labels'] = len(sent_labels)\n",
    "for tag in ['train', 'val', 'test']:\n",
    "    kws_config['{}_manifest'.format(tag)] =  os.path.join(manifest_dir,'kws_{}_{}_manifest.csv'.format(\"command\", tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kws_model = kws_config['model_class'](kws_config)\n",
    "# kws_model.load(config['output_file'])\n",
    "kws_config['input_file'] = \"\"\n",
    "kws_config['output_file'] = \"models/kws_command_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.train(kws_config, model=kws_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kws_model.load(\"models/kws_command_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kws train with pretrained si model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kws on the si\n",
    "import itertools\n",
    "\n",
    "si_state = si_model.state_dict()\n",
    "fine_tuned_params = ['output.weight', 'output.bias', 'bottleneck.weight', 'bottlenec.bias']\n",
    "pre_trained_state = {k: v for k, v in si_state.items() if k not in fine_tuned_params}\n",
    "\n",
    "kws_si_config = kws_config.copy()\n",
    "kws_si_model = kws_si_config['model_class'](kws_si_config)\n",
    "kws_state = kws_si_model.state_dict()\n",
    "kws_state.update(pre_trained_state)\n",
    "kws_si_model.load_state_dict(kws_state)  \n",
    "\n",
    "for param in kws_si_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in itertools.chain(kws_si_model.output.parameters(), kws_si_model.bottleneck.parameters()):\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kws_si_config['n_epochs'] = 50 # it requires realatively more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step #99 accuracy: 0.28125, loss: 2.359569787979126\n",
      "train step #199 accuracy: 0.1875, loss: 2.311204195022583\n",
      "train step #299 accuracy: 0.28125, loss: 2.3460240364074707\n",
      "train step #399 accuracy: 0.40625, loss: 1.910219669342041\n",
      "train step #499 accuracy: 0.40625, loss: 2.072744607925415\n",
      "train step #599 accuracy: 0.3125, loss: 2.069772243499756\n",
      "train step #699 accuracy: 0.34375, loss: 2.2566230297088623\n",
      "train step #799 accuracy: 0.4375, loss: 1.765905499458313\n",
      "train step #899 accuracy: 0.4375, loss: 1.765110969543457\n",
      "train step #999 accuracy: 0.25, loss: 1.9999923706054688\n",
      "train step #1099 accuracy: 0.40625, loss: 1.9194960594177246\n",
      "epoch #0, final dev accuracy: 0.5159676535087719\n",
      "saving best model...\n",
      "train step #1199 accuracy: 0.65625, loss: 1.4000054597854614\n",
      "train step #1299 accuracy: 0.53125, loss: 1.8892356157302856\n",
      "train step #1399 accuracy: 0.59375, loss: 1.55961275100708\n",
      "train step #1499 accuracy: 0.4375, loss: 1.8440595865249634\n",
      "train step #1599 accuracy: 0.53125, loss: 1.560726523399353\n",
      "train step #1699 accuracy: 0.3125, loss: 1.849320650100708\n",
      "train step #1799 accuracy: 0.5, loss: 1.5329341888427734\n",
      "train step #1899 accuracy: 0.40625, loss: 1.9978350400924683\n",
      "train step #1999 accuracy: 0.375, loss: 2.008903741836548\n",
      "train step #2099 accuracy: 0.53125, loss: 1.5920119285583496\n",
      "train step #2199 accuracy: 0.46875, loss: 1.4289231300354004\n",
      "epoch #1, final dev accuracy: 0.5374177631578947\n",
      "saving best model...\n",
      "train step #2299 accuracy: 0.4375, loss: 1.8216663599014282\n",
      "train step #2399 accuracy: 0.625, loss: 1.2197730541229248\n",
      "train step #2499 accuracy: 0.4375, loss: 1.4355405569076538\n",
      "train step #2599 accuracy: 0.625, loss: 1.325999140739441\n",
      "train step #2699 accuracy: 0.53125, loss: 1.3856366872787476\n",
      "train step #2799 accuracy: 0.5, loss: 1.7915273904800415\n",
      "train step #2899 accuracy: 0.5, loss: 1.8434579372406006\n",
      "train step #2999 accuracy: 0.4375, loss: 1.5970035791397095\n",
      "train step #3099 accuracy: 0.46875, loss: 2.068051338195801\n",
      "train step #3199 accuracy: 0.40625, loss: 2.0760841369628906\n",
      "train step #3299 accuracy: 0.5625, loss: 1.4138370752334595\n",
      "epoch #2, final dev accuracy: 0.5512609649122807\n",
      "saving best model...\n",
      "train step #3399 accuracy: 0.59375, loss: 1.3804888725280762\n",
      "train step #3499 accuracy: 0.5, loss: 1.4051512479782104\n",
      "train step #3599 accuracy: 0.40625, loss: 2.1363816261291504\n",
      "train step #3699 accuracy: 0.65625, loss: 1.1755058765411377\n",
      "train step #3799 accuracy: 0.53125, loss: 1.4993177652359009\n",
      "train step #3899 accuracy: 0.78125, loss: 0.8748286366462708\n",
      "train step #3999 accuracy: 0.34375, loss: 1.784541368484497\n",
      "train step #4099 accuracy: 0.40625, loss: 1.767516851425171\n",
      "train step #4199 accuracy: 0.46875, loss: 1.4982197284698486\n",
      "train step #4299 accuracy: 0.625, loss: 1.4701006412506104\n",
      "train step #4399 accuracy: 0.53125, loss: 1.7417234182357788\n",
      "train step #4499 accuracy: 0.25, loss: 2.258880615234375\n",
      "epoch #3, final dev accuracy: 0.5584566885964912\n",
      "saving best model...\n",
      "train step #4599 accuracy: 0.4375, loss: 1.9425934553146362\n",
      "train step #4699 accuracy: 0.75, loss: 0.9427456855773926\n",
      "train step #4799 accuracy: 0.53125, loss: 1.5331345796585083\n",
      "train step #4899 accuracy: 0.625, loss: 1.3013916015625\n",
      "train step #4999 accuracy: 0.5625, loss: 1.7819441556930542\n",
      "train step #5099 accuracy: 0.53125, loss: 1.894675612449646\n",
      "train step #5199 accuracy: 0.34375, loss: 1.7864781618118286\n",
      "train step #5299 accuracy: 0.71875, loss: 1.3689454793930054\n",
      "train step #5399 accuracy: 0.53125, loss: 1.7118258476257324\n",
      "train step #5499 accuracy: 0.59375, loss: 1.511211633682251\n",
      "train step #5599 accuracy: 0.5, loss: 1.665623664855957\n",
      "epoch #4, final dev accuracy: 0.5770285087719298\n",
      "saving best model...\n",
      "train step #5699 accuracy: 0.53125, loss: 1.1701699495315552\n",
      "train step #5799 accuracy: 0.625, loss: 1.555009365081787\n",
      "train step #5899 accuracy: 0.53125, loss: 2.036400556564331\n",
      "train step #5999 accuracy: 0.375, loss: 2.1028997898101807\n",
      "train step #6099 accuracy: 0.5, loss: 2.085336685180664\n",
      "train step #6199 accuracy: 0.59375, loss: 1.2395892143249512\n",
      "train step #6299 accuracy: 0.59375, loss: 1.3107573986053467\n",
      "train step #6399 accuracy: 0.46875, loss: 2.026078939437866\n",
      "train step #6499 accuracy: 0.53125, loss: 1.4907100200653076\n",
      "train step #6599 accuracy: 0.46875, loss: 1.6063361167907715\n",
      "train step #6699 accuracy: 0.5625, loss: 1.741513967514038\n",
      "epoch #5, final dev accuracy: 0.5465323464912281\n",
      "train step #6799 accuracy: 0.53125, loss: 1.9421827793121338\n",
      "train step #6899 accuracy: 0.53125, loss: 1.670663595199585\n",
      "train step #6999 accuracy: 0.40625, loss: 1.866037368774414\n",
      "train step #7099 accuracy: 0.40625, loss: 1.852528691291809\n",
      "train step #7199 accuracy: 0.59375, loss: 1.539105772972107\n",
      "train step #7299 accuracy: 0.65625, loss: 1.1303263902664185\n",
      "train step #7399 accuracy: 0.53125, loss: 1.3604192733764648\n",
      "train step #7499 accuracy: 0.53125, loss: 1.712443232536316\n",
      "train step #7599 accuracy: 0.40625, loss: 2.0030674934387207\n",
      "train step #7699 accuracy: 0.59375, loss: 1.8450193405151367\n",
      "train step #7799 accuracy: 0.53125, loss: 1.569746494293213\n",
      "train step #7899 accuracy: 0.59375, loss: 1.3542884588241577\n",
      "epoch #6, final dev accuracy: 0.5439281798245613\n",
      "train step #7999 accuracy: 0.59375, loss: 2.1124300956726074\n",
      "train step #8099 accuracy: 0.5625, loss: 1.4818994998931885\n",
      "train step #8199 accuracy: 0.5625, loss: 0.9593650102615356\n",
      "train step #8299 accuracy: 0.59375, loss: 1.6661884784698486\n",
      "train step #8399 accuracy: 0.625, loss: 1.415037751197815\n",
      "train step #8499 accuracy: 0.65625, loss: 1.15442955493927\n",
      "train step #8599 accuracy: 0.5, loss: 1.578866720199585\n",
      "train step #8699 accuracy: 0.5, loss: 1.95320725440979\n",
      "train step #8799 accuracy: 0.53125, loss: 1.606704592704773\n",
      "train step #8899 accuracy: 0.59375, loss: 1.4375051259994507\n",
      "train step #8999 accuracy: 0.375, loss: 1.8611963987350464\n",
      "epoch #7, final dev accuracy: 0.5651041666666666\n",
      "train step #9099 accuracy: 0.5625, loss: 1.5682624578475952\n",
      "train step #9199 accuracy: 0.40625, loss: 1.9506514072418213\n",
      "train step #9299 accuracy: 0.625, loss: 1.0056581497192383\n",
      "train step #9399 accuracy: 0.4375, loss: 1.4877804517745972\n",
      "train step #9499 accuracy: 0.5625, loss: 1.5411663055419922\n",
      "train step #9599 accuracy: 0.53125, loss: 1.6965445280075073\n",
      "train step #9699 accuracy: 0.5625, loss: 1.3844850063323975\n",
      "train step #9799 accuracy: 0.53125, loss: 1.5723958015441895\n",
      "train step #9899 accuracy: 0.5625, loss: 1.440375566482544\n",
      "train step #9999 accuracy: 0.5, loss: 1.867897868156433\n",
      "train step #10099 accuracy: 0.53125, loss: 2.1160850524902344\n",
      "epoch #8, final dev accuracy: 0.5470120614035088\n",
      "train step #10199 accuracy: 0.5625, loss: 1.7789382934570312\n",
      "train step #10299 accuracy: 0.53125, loss: 1.6061971187591553\n",
      "train step #10399 accuracy: 0.65625, loss: 1.13412344455719\n",
      "train step #10499 accuracy: 0.59375, loss: 1.1402311325073242\n",
      "train step #10599 accuracy: 0.625, loss: 1.5235140323638916\n",
      "train step #10699 accuracy: 0.625, loss: 1.459967017173767\n",
      "train step #10799 accuracy: 0.71875, loss: 0.9593479633331299\n",
      "train step #10899 accuracy: 0.59375, loss: 1.4053112268447876\n",
      "train step #10999 accuracy: 0.53125, loss: 1.6459269523620605\n",
      "train step #11099 accuracy: 0.3125, loss: 2.131321668624878\n",
      "train step #11199 accuracy: 0.59375, loss: 1.4417654275894165\n",
      "train step #11299 accuracy: 0.59375, loss: 1.3869868516921997\n",
      "epoch #9, final dev accuracy: 0.5550986842105263\n",
      "train step #11399 accuracy: 0.625, loss: 1.0551068782806396\n",
      "train step #11499 accuracy: 0.625, loss: 1.4103410243988037\n",
      "train step #11599 accuracy: 0.59375, loss: 1.151714563369751\n",
      "train step #11699 accuracy: 0.53125, loss: 1.4144279956817627\n",
      "train step #11799 accuracy: 0.6875, loss: 1.1926472187042236\n",
      "train step #11899 accuracy: 0.5625, loss: 1.5569056272506714\n",
      "train step #11999 accuracy: 0.75, loss: 0.8738351464271545\n",
      "train step #12099 accuracy: 0.5, loss: 1.557956576347351\n",
      "train step #12199 accuracy: 0.59375, loss: 1.4138062000274658\n",
      "train step #12299 accuracy: 0.53125, loss: 2.157687187194824\n",
      "train step #12399 accuracy: 0.625, loss: 1.6243947744369507\n",
      "epoch #10, final dev accuracy: 0.5617461622807017\n",
      "train step #12499 accuracy: 0.5625, loss: 1.6403429508209229\n",
      "train step #12599 accuracy: 0.5, loss: 1.8737772703170776\n",
      "train step #12699 accuracy: 0.53125, loss: 1.5431568622589111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step #12799 accuracy: 0.65625, loss: 1.3565542697906494\n",
      "train step #12899 accuracy: 0.5625, loss: 1.2155256271362305\n",
      "train step #12999 accuracy: 0.6875, loss: 1.0276720523834229\n",
      "train step #13099 accuracy: 0.625, loss: 1.4967015981674194\n",
      "train step #13199 accuracy: 0.625, loss: 1.3414160013198853\n",
      "train step #13299 accuracy: 0.53125, loss: 1.3838539123535156\n",
      "train step #13399 accuracy: 0.53125, loss: 1.7438846826553345\n",
      "train step #13499 accuracy: 0.625, loss: 1.4318907260894775\n",
      "epoch #11, final dev accuracy: 0.5474917763157895\n",
      "train step #13599 accuracy: 0.6875, loss: 1.3885750770568848\n",
      "train step #13699 accuracy: 0.59375, loss: 1.2471654415130615\n",
      "train step #13799 accuracy: 0.5625, loss: 1.4361377954483032\n",
      "train step #13899 accuracy: 0.65625, loss: 1.2630186080932617\n",
      "train step #13999 accuracy: 0.625, loss: 1.4677655696868896\n",
      "train step #14099 accuracy: 0.53125, loss: 1.8813350200653076\n",
      "train step #14199 accuracy: 0.46875, loss: 1.409987449645996\n",
      "train step #14299 accuracy: 0.71875, loss: 0.9659996628761292\n",
      "train step #14399 accuracy: 0.4375, loss: 1.8554778099060059\n",
      "train step #14499 accuracy: 0.46875, loss: 2.583489418029785\n",
      "train step #14599 accuracy: 0.6875, loss: 1.118864893913269\n",
      "train step #14699 accuracy: 0.5, loss: 1.6533424854278564\n",
      "epoch #12, final dev accuracy: 0.5722998903508771\n",
      "train step #14799 accuracy: 0.65625, loss: 1.5556540489196777\n",
      "train step #14899 accuracy: 0.59375, loss: 1.3553804159164429\n",
      "train step #14999 accuracy: 0.625, loss: 1.3378310203552246\n",
      "train step #15099 accuracy: 0.5625, loss: 1.5320053100585938\n",
      "train step #15199 accuracy: 0.65625, loss: 1.242983341217041\n",
      "train step #15299 accuracy: 0.46875, loss: 1.3327041864395142\n",
      "train step #15399 accuracy: 0.59375, loss: 1.0700795650482178\n",
      "train step #15499 accuracy: 0.40625, loss: 1.6984379291534424\n",
      "train step #15599 accuracy: 0.5, loss: 1.788153052330017\n",
      "train step #15699 accuracy: 0.40625, loss: 1.9595530033111572\n",
      "train step #15799 accuracy: 0.5, loss: 1.3217355012893677\n",
      "epoch #13, final dev accuracy: 0.5461896929824561\n",
      "train step #15899 accuracy: 0.65625, loss: 1.255266785621643\n",
      "train step #15999 accuracy: 0.53125, loss: 1.8541948795318604\n",
      "train step #16099 accuracy: 0.5, loss: 1.4850019216537476\n",
      "train step #16199 accuracy: 0.5, loss: 1.4450135231018066\n",
      "train step #16299 accuracy: 0.625, loss: 1.3910514116287231\n",
      "train step #16399 accuracy: 0.53125, loss: 1.4568623304367065\n",
      "train step #16499 accuracy: 0.5625, loss: 1.3182780742645264\n",
      "train step #16599 accuracy: 0.5625, loss: 1.1325280666351318\n",
      "train step #16699 accuracy: 0.625, loss: 1.5216461420059204\n",
      "train step #16799 accuracy: 0.53125, loss: 1.8825063705444336\n",
      "train step #16899 accuracy: 0.53125, loss: 1.3976593017578125\n",
      "epoch #14, final dev accuracy: 0.561266447368421\n",
      "train step #16999 accuracy: 0.46875, loss: 1.6646740436553955\n",
      "train step #17099 accuracy: 0.375, loss: 2.1008453369140625\n",
      "train step #17199 accuracy: 0.59375, loss: 1.1076288223266602\n",
      "train step #17299 accuracy: 0.5625, loss: 1.1726622581481934\n",
      "train step #17399 accuracy: 0.5625, loss: 1.407371163368225\n",
      "train step #17499 accuracy: 0.53125, loss: 1.6237047910690308\n",
      "train step #17599 accuracy: 0.4375, loss: 1.6013044118881226\n",
      "train step #17699 accuracy: 0.65625, loss: 1.1806727647781372\n",
      "train step #17799 accuracy: 0.6875, loss: 1.049534559249878\n",
      "train step #17899 accuracy: 0.6875, loss: 1.279421091079712\n",
      "train step #17999 accuracy: 0.59375, loss: 1.4551993608474731\n",
      "epoch #15, final dev accuracy: 0.5572916666666666\n",
      "train step #18099 accuracy: 0.5625, loss: 1.4948590993881226\n",
      "train step #18199 accuracy: 0.5, loss: 1.7895081043243408\n",
      "train step #18299 accuracy: 0.5, loss: 1.9222888946533203\n",
      "train step #18399 accuracy: 0.6875, loss: 1.3752498626708984\n",
      "train step #18499 accuracy: 0.65625, loss: 1.1122244596481323\n",
      "train step #18599 accuracy: 0.5, loss: 1.877144455909729\n",
      "train step #18699 accuracy: 0.4375, loss: 2.045539617538452\n",
      "train step #18799 accuracy: 0.5625, loss: 1.5639777183532715\n",
      "train step #18899 accuracy: 0.4375, loss: 2.5631208419799805\n",
      "train step #18999 accuracy: 0.59375, loss: 1.2216519117355347\n",
      "train step #19099 accuracy: 0.5, loss: 1.7004878520965576\n",
      "train step #19199 accuracy: 0.53125, loss: 1.3353896141052246\n",
      "epoch #16, final dev accuracy: 0.5368009868421053\n",
      "train step #19299 accuracy: 0.625, loss: 1.0613738298416138\n",
      "train step #19399 accuracy: 0.5625, loss: 1.8385155200958252\n",
      "train step #19499 accuracy: 0.40625, loss: 1.8817405700683594\n",
      "train step #19599 accuracy: 0.65625, loss: 1.4227728843688965\n",
      "train step #19699 accuracy: 0.53125, loss: 1.3689885139465332\n",
      "train step #19799 accuracy: 0.46875, loss: 2.038408041000366\n",
      "train step #19899 accuracy: 0.6875, loss: 1.2398802042007446\n",
      "train step #19999 accuracy: 0.6875, loss: 1.3543109893798828\n",
      "train step #20099 accuracy: 0.5625, loss: 1.7517814636230469\n",
      "train step #20199 accuracy: 0.40625, loss: 1.7875444889068604\n",
      "train step #20299 accuracy: 0.53125, loss: 1.8308844566345215\n",
      "epoch #17, final dev accuracy: 0.5459155701754387\n",
      "train step #20399 accuracy: 0.71875, loss: 0.7630480527877808\n",
      "train step #20499 accuracy: 0.625, loss: 1.1423578262329102\n",
      "train step #20599 accuracy: 0.5, loss: 1.5300922393798828\n",
      "train step #20699 accuracy: 0.625, loss: 1.1390131711959839\n",
      "train step #20799 accuracy: 0.71875, loss: 0.9376742243766785\n",
      "train step #20899 accuracy: 0.4375, loss: 2.375128984451294\n",
      "train step #20999 accuracy: 0.53125, loss: 1.7192171812057495\n",
      "train step #21099 accuracy: 0.5, loss: 1.717209815979004\n",
      "train step #21199 accuracy: 0.53125, loss: 2.026254415512085\n",
      "train step #21299 accuracy: 0.5, loss: 1.4569449424743652\n",
      "train step #21399 accuracy: 0.5625, loss: 1.4852555990219116\n",
      "epoch #18, final dev accuracy: 0.552563048245614\n",
      "train step #21499 accuracy: 0.40625, loss: 1.4474350214004517\n",
      "train step #21599 accuracy: 0.5, loss: 1.8131952285766602\n",
      "train step #21699 accuracy: 0.5625, loss: 1.6878048181533813\n",
      "train step #21799 accuracy: 0.6875, loss: 1.3498886823654175\n",
      "train step #21899 accuracy: 0.59375, loss: 1.3974970579147339\n",
      "train step #21999 accuracy: 0.625, loss: 1.2098917961120605\n",
      "train step #22099 accuracy: 0.5, loss: 1.4084222316741943\n",
      "train step #22199 accuracy: 0.6875, loss: 0.9229880571365356\n",
      "train step #22299 accuracy: 0.6875, loss: 0.9504244327545166\n",
      "train step #22399 accuracy: 0.46875, loss: 1.6187822818756104\n",
      "train step #22499 accuracy: 0.65625, loss: 1.1603436470031738\n",
      "train step #22599 accuracy: 0.59375, loss: 1.837558627128601\n",
      "epoch #19, final dev accuracy: 0.5406387061403508\n",
      "train step #22699 accuracy: 0.625, loss: 1.271950602531433\n",
      "train step #22799 accuracy: 0.4375, loss: 1.7163708209991455\n",
      "train step #22899 accuracy: 0.46875, loss: 2.2896013259887695\n",
      "train step #22999 accuracy: 0.59375, loss: 1.823314905166626\n",
      "train step #23099 accuracy: 0.53125, loss: 1.2118545770645142\n",
      "train step #23199 accuracy: 0.625, loss: 1.1755447387695312\n",
      "train step #23299 accuracy: 0.6875, loss: 1.2950252294540405\n",
      "train step #23399 accuracy: 0.53125, loss: 1.7931910753250122\n",
      "train step #23499 accuracy: 0.53125, loss: 1.2690074443817139\n",
      "train step #23599 accuracy: 0.625, loss: 1.6711937189102173\n",
      "train step #23699 accuracy: 0.4375, loss: 1.8732728958129883\n",
      "epoch #20, final dev accuracy: 0.5540021929824561\n",
      "train step #23799 accuracy: 0.6875, loss: 1.2832142114639282\n",
      "train step #23899 accuracy: 0.625, loss: 1.565988540649414\n",
      "train step #23999 accuracy: 0.625, loss: 1.8442031145095825\n",
      "train step #24099 accuracy: 0.625, loss: 1.354947805404663\n",
      "train step #24199 accuracy: 0.59375, loss: 1.5623576641082764\n",
      "train step #24299 accuracy: 0.625, loss: 1.6472375392913818\n",
      "train step #24399 accuracy: 0.5, loss: 1.238835096359253\n",
      "train step #24499 accuracy: 0.5, loss: 1.7576756477355957\n",
      "train step #24599 accuracy: 0.46875, loss: 1.7061593532562256\n",
      "train step #24699 accuracy: 0.59375, loss: 1.8022074699401855\n",
      "train step #24799 accuracy: 0.625, loss: 1.2021766901016235\n",
      "epoch #21, final dev accuracy: 0.560375548245614\n",
      "train step #24899 accuracy: 0.59375, loss: 1.68563973903656\n",
      "train step #24999 accuracy: 0.625, loss: 1.5206243991851807\n",
      "train step #25099 accuracy: 0.59375, loss: 1.229406476020813\n",
      "train step #25199 accuracy: 0.65625, loss: 1.3301736116409302\n",
      "train step #25299 accuracy: 0.375, loss: 2.415447235107422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step #25399 accuracy: 0.5625, loss: 1.802772879600525\n",
      "train step #25499 accuracy: 0.46875, loss: 1.8677194118499756\n",
      "train step #25599 accuracy: 0.5, loss: 1.7388845682144165\n",
      "train step #25699 accuracy: 0.53125, loss: 1.6652553081512451\n",
      "train step #25799 accuracy: 0.53125, loss: 1.8573520183563232\n",
      "train step #25899 accuracy: 0.65625, loss: 1.4448773860931396\n",
      "train step #25999 accuracy: 0.625, loss: 1.2306256294250488\n",
      "epoch #22, final dev accuracy: 0.5577713815789473\n",
      "train step #26099 accuracy: 0.34375, loss: 2.066279649734497\n",
      "train step #26199 accuracy: 0.65625, loss: 1.2866032123565674\n",
      "train step #26299 accuracy: 0.59375, loss: 1.413832187652588\n",
      "train step #26399 accuracy: 0.4375, loss: 2.023177146911621\n",
      "train step #26499 accuracy: 0.71875, loss: 1.0513980388641357\n",
      "train step #26599 accuracy: 0.5625, loss: 1.107807993888855\n",
      "train step #26699 accuracy: 0.4375, loss: 1.659938931465149\n",
      "train step #26799 accuracy: 0.53125, loss: 1.5425199270248413\n",
      "train step #26899 accuracy: 0.5, loss: 1.6726765632629395\n",
      "train step #26999 accuracy: 0.53125, loss: 1.303401231765747\n",
      "train step #27099 accuracy: 0.53125, loss: 1.508738398551941\n",
      "epoch #23, final dev accuracy: 0.5524945175438597\n",
      "train step #27199 accuracy: 0.625, loss: 1.0684646368026733\n",
      "train step #27299 accuracy: 0.59375, loss: 1.3259162902832031\n",
      "train step #27399 accuracy: 0.4375, loss: 1.9009279012680054\n",
      "train step #27499 accuracy: 0.59375, loss: 1.3883295059204102\n",
      "train step #27599 accuracy: 0.625, loss: 1.7183878421783447\n",
      "train step #27699 accuracy: 0.5, loss: 1.7808868885040283\n",
      "train step #27799 accuracy: 0.65625, loss: 1.4072847366333008\n",
      "train step #27899 accuracy: 0.46875, loss: 1.9630110263824463\n",
      "train step #27999 accuracy: 0.59375, loss: 1.007914662361145\n",
      "train step #28099 accuracy: 0.46875, loss: 1.8544814586639404\n",
      "train step #28199 accuracy: 0.4375, loss: 1.8039827346801758\n",
      "epoch #24, final dev accuracy: 0.5703810307017544\n",
      "train step #28299 accuracy: 0.46875, loss: 1.4518840312957764\n",
      "train step #28399 accuracy: 0.65625, loss: 1.2537295818328857\n",
      "train step #28499 accuracy: 0.5625, loss: 1.7215898036956787\n",
      "train step #28599 accuracy: 0.625, loss: 1.6137440204620361\n",
      "train step #28699 accuracy: 0.65625, loss: 1.5206527709960938\n",
      "train step #28799 accuracy: 0.59375, loss: 1.226387619972229\n",
      "train step #28899 accuracy: 0.53125, loss: 1.8541796207427979\n",
      "train step #28999 accuracy: 0.59375, loss: 1.381601333618164\n",
      "train step #29099 accuracy: 0.5625, loss: 1.6908605098724365\n",
      "train step #29199 accuracy: 0.53125, loss: 1.8664226531982422\n",
      "train step #29299 accuracy: 0.6875, loss: 1.4066978693008423\n",
      "train step #29399 accuracy: 0.53125, loss: 1.8464637994766235\n",
      "epoch #25, final dev accuracy: 0.5483826754385965\n",
      "train step #29499 accuracy: 0.625, loss: 1.610756278038025\n",
      "train step #29599 accuracy: 0.375, loss: 2.1035571098327637\n",
      "train step #29699 accuracy: 0.46875, loss: 1.7760499715805054\n",
      "train step #29799 accuracy: 0.5, loss: 2.248818874359131\n",
      "train step #29899 accuracy: 0.4375, loss: 1.6322760581970215\n",
      "train step #29999 accuracy: 0.5, loss: 1.966399908065796\n",
      "train step #30099 accuracy: 0.65625, loss: 1.5368945598602295\n",
      "train step #30199 accuracy: 0.65625, loss: 1.6259548664093018\n",
      "train step #30299 accuracy: 0.5, loss: 2.2835960388183594\n",
      "train step #30399 accuracy: 0.375, loss: 1.9290671348571777\n",
      "train step #30499 accuracy: 0.40625, loss: 1.9194331169128418\n",
      "epoch #26, final dev accuracy: 0.5518092105263158\n",
      "train step #30599 accuracy: 0.59375, loss: 1.8809239864349365\n",
      "train step #30699 accuracy: 0.53125, loss: 1.6271772384643555\n",
      "train step #30799 accuracy: 0.625, loss: 1.7930335998535156\n",
      "train step #30899 accuracy: 0.65625, loss: 1.1002098321914673\n",
      "train step #30999 accuracy: 0.5, loss: 2.024183750152588\n",
      "train step #31099 accuracy: 0.5, loss: 2.0480525493621826\n",
      "train step #31199 accuracy: 0.5625, loss: 1.545827031135559\n",
      "train step #31299 accuracy: 0.46875, loss: 1.509529948234558\n",
      "train step #31399 accuracy: 0.5625, loss: 1.3006393909454346\n",
      "train step #31499 accuracy: 0.375, loss: 2.5605814456939697\n",
      "train step #31599 accuracy: 0.3125, loss: 2.6697375774383545\n",
      "epoch #27, final dev accuracy: 0.5483826754385965\n",
      "train step #31699 accuracy: 0.40625, loss: 2.1534886360168457\n",
      "train step #31799 accuracy: 0.625, loss: 1.4247148036956787\n",
      "train step #31899 accuracy: 0.59375, loss: 1.7258939743041992\n",
      "train step #31999 accuracy: 0.375, loss: 2.0067992210388184\n",
      "train step #32099 accuracy: 0.46875, loss: 1.8070534467697144\n",
      "train step #32199 accuracy: 0.5, loss: 1.7480815649032593\n",
      "train step #32299 accuracy: 0.65625, loss: 1.3512332439422607\n",
      "train step #32399 accuracy: 0.53125, loss: 1.7896195650100708\n",
      "train step #32499 accuracy: 0.53125, loss: 1.6353049278259277\n",
      "train step #32599 accuracy: 0.34375, loss: 2.307194709777832\n",
      "train step #32699 accuracy: 0.65625, loss: 1.3759229183197021\n",
      "train step #32799 accuracy: 0.53125, loss: 1.3413647413253784\n",
      "epoch #28, final dev accuracy: 0.5532483552631579\n",
      "train step #32899 accuracy: 0.65625, loss: 1.1026685237884521\n",
      "train step #32999 accuracy: 0.59375, loss: 1.6314517259597778\n",
      "train step #33099 accuracy: 0.53125, loss: 1.4264410734176636\n",
      "train step #33199 accuracy: 0.5625, loss: 1.5090833902359009\n",
      "train step #33299 accuracy: 0.4375, loss: 1.896555781364441\n",
      "train step #33399 accuracy: 0.5, loss: 1.817150354385376\n",
      "train step #33499 accuracy: 0.46875, loss: 1.9849865436553955\n",
      "train step #33599 accuracy: 0.59375, loss: 1.763586401939392\n",
      "train step #33699 accuracy: 0.34375, loss: 2.103168487548828\n",
      "train step #33799 accuracy: 0.4375, loss: 2.364617109298706\n",
      "train step #33899 accuracy: 0.5625, loss: 1.7246173620224\n",
      "epoch #29, final dev accuracy: 0.5727796052631579\n",
      "final test accuracy: 0.529928172386273\n"
     ]
    }
   ],
   "source": [
    "hk.train(kws_si_config, model=kws_si_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTL learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling\n",
    "mtl_train = si_df[si_df.sent.isin(main_sents)].sample(frac=0.8)\n",
    "mtl_test = si_df[si_df.sent.isin(main_sents)].drop(index=mtl_train.index)\n",
    "mtl_val = mtl_test.sample(frac=0.5)\n",
    "mtl_test = mtl_test.drop(index=mtl_val.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifests/commands/mtl/mtl_command_train_manifest.csv was written\n",
      "manifests/commands/mtl/mtl_command_val_manifest.csv was written\n",
      "manifests/commands/mtl/mtl_command_test_manifest.csv was written\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "tags = ['train', 'val', 'test']\n",
    "sets = {'train':mtl_train, 'val':mtl_val, 'test':mtl_test}\n",
    "manifest_dir = \"manifests/commands/mtl\"\n",
    "\n",
    "for tag in tags:\n",
    "    samples = []\n",
    "    save_path = os.path.join(manifest_dir,'mtl_{}_{}_manifest.csv'.format(\"command\", tag))\n",
    "    with open(save_path, 'w') as f:\n",
    "        for index, row in sets[tag].iterrows():\n",
    "            file_path = os.path.join(data_dir, row.sent, row.file)\n",
    "            spk_label = row.label\n",
    "            sent_label = sent_labels.index(row.sent)\n",
    "            sample = ','.join([file_path, str(spk_label), str(sent_label)])\n",
    "            samples.append(sample)\n",
    "        random.shuffle(samples)\n",
    "        writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "        writer.writerow(samples)\n",
    "        print(\"{} was written\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_config = si_config.copy()\n",
    "mtl_config['n_labels'] = 2000\n",
    "mtl_config['n_labels1'] = len(sent_labels)\n",
    "mtl_config['model_class'] = mod.MTLSpeechModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_dir = \"manifests/commands/mtl\"\n",
    "for tag in ['train', 'val', 'test']:\n",
    "    mtl_config['{}_manifest'.format(tag)] =  os.path.join(manifest_dir,\n",
    "                                                          'mtl_{}_{}_manifest.csv'.format(\"command\", tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_config['input_file'] = \"\"\n",
    "mtl_config['output_file'] = \"models/mtl_command_model.pt\"\n",
    "mtl_config['alpha'] = 0.7\n",
    "mtl_config['n_epochs'] = 15\n",
    "mtl_model = mtl_config['model_class'](mtl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spk] train step #99 accuracy: 0.0, loss: 6.085064888000488\n",
      "[sent] train step #99 accuracy: 0.125, loss: 6.085064888000488\n",
      "[spk] train step #199 accuracy: 0.0, loss: 5.689327239990234\n",
      "[sent] train step #199 accuracy: 0.40625, loss: 5.689327239990234\n",
      "[spk] train step #299 accuracy: 0.0, loss: 5.8006272315979\n",
      "[sent] train step #299 accuracy: 0.34375, loss: 5.8006272315979\n",
      "[spk] train step #399 accuracy: 0.0, loss: 5.747798442840576\n",
      "[sent] train step #399 accuracy: 0.3125, loss: 5.747798442840576\n",
      "[spk] train step #499 accuracy: 0.0, loss: 5.4137187004089355\n",
      "[sent] train step #499 accuracy: 0.3125, loss: 5.4137187004089355\n",
      "[spk] train step #599 accuracy: 0.03125, loss: 5.3870697021484375\n",
      "[sent] train step #599 accuracy: 0.5, loss: 5.3870697021484375\n",
      "[spk] train step #699 accuracy: 0.03125, loss: 5.1953582763671875\n",
      "[sent] train step #699 accuracy: 0.3125, loss: 5.1953582763671875\n",
      "[spk] train step #799 accuracy: 0.0625, loss: 5.313719749450684\n",
      "[sent] train step #799 accuracy: 0.3125, loss: 5.313719749450684\n",
      "[spk] train step #899 accuracy: 0.09375, loss: 4.926460266113281\n",
      "[sent] train step #899 accuracy: 0.40625, loss: 4.926460266113281\n",
      "[spk] train step #999 accuracy: 0.125, loss: 4.447755813598633\n",
      "[sent] train step #999 accuracy: 0.46875, loss: 4.447755813598633\n",
      "[spk] train step #1099 accuracy: 0.09375, loss: 4.338599681854248\n",
      "[sent] train step #1099 accuracy: 0.46875, loss: 4.338599681854248\n",
      "epoch #0, final dev accuracy: 0.1261200716845878, 0.48320592517921146\n",
      "saving best model...\n",
      "[spk] train step #1199 accuracy: 0.125, loss: 4.304018020629883\n",
      "[sent] train step #1199 accuracy: 0.5, loss: 4.304018020629883\n",
      "[spk] train step #1299 accuracy: 0.1875, loss: 4.060643672943115\n",
      "[sent] train step #1299 accuracy: 0.375, loss: 4.060643672943115\n",
      "[spk] train step #1399 accuracy: 0.03125, loss: 3.7618749141693115\n",
      "[sent] train step #1399 accuracy: 0.46875, loss: 3.7618749141693115\n",
      "[spk] train step #1499 accuracy: 0.3125, loss: 3.121159791946411\n",
      "[sent] train step #1499 accuracy: 0.65625, loss: 3.121159791946411\n",
      "[spk] train step #1599 accuracy: 0.3125, loss: 3.284512519836426\n",
      "[sent] train step #1599 accuracy: 0.59375, loss: 3.284512519836426\n",
      "[spk] train step #1699 accuracy: 0.28125, loss: 3.1096439361572266\n",
      "[sent] train step #1699 accuracy: 0.4375, loss: 3.1096439361572266\n",
      "[spk] train step #1799 accuracy: 0.15625, loss: 3.4099783897399902\n",
      "[sent] train step #1799 accuracy: 0.46875, loss: 3.4099783897399902\n",
      "[spk] train step #1899 accuracy: 0.40625, loss: 2.922041893005371\n",
      "[sent] train step #1899 accuracy: 0.5, loss: 2.922041893005371\n",
      "[spk] train step #1999 accuracy: 0.34375, loss: 2.391108512878418\n",
      "[sent] train step #1999 accuracy: 0.6875, loss: 2.391108512878418\n",
      "[spk] train step #2099 accuracy: 0.5, loss: 2.484074115753174\n",
      "[sent] train step #2099 accuracy: 0.625, loss: 2.484074115753174\n",
      "[spk] train step #2199 accuracy: 0.28125, loss: 3.341071128845215\n",
      "[sent] train step #2199 accuracy: 0.5, loss: 3.341071128845215\n",
      "[spk] train step #2299 accuracy: 0.4375, loss: 2.382425546646118\n",
      "[sent] train step #2299 accuracy: 0.59375, loss: 2.382425546646118\n",
      "epoch #1, final dev accuracy: 0.4301985327060932, 0.6158014112903225\n",
      "[spk] train step #2399 accuracy: 0.5625, loss: 1.8711882829666138\n",
      "[sent] train step #2399 accuracy: 0.75, loss: 1.8711882829666138\n",
      "[spk] train step #2499 accuracy: 0.46875, loss: 2.1818854808807373\n",
      "[sent] train step #2499 accuracy: 0.53125, loss: 2.1818854808807373\n",
      "[spk] train step #2599 accuracy: 0.5, loss: 2.010296583175659\n",
      "[sent] train step #2599 accuracy: 0.5625, loss: 2.010296583175659\n",
      "[spk] train step #2699 accuracy: 0.5, loss: 2.217229127883911\n",
      "[sent] train step #2699 accuracy: 0.4375, loss: 2.217229127883911\n",
      "[spk] train step #2799 accuracy: 0.53125, loss: 2.0047922134399414\n",
      "[sent] train step #2799 accuracy: 0.625, loss: 2.0047922134399414\n",
      "[spk] train step #2899 accuracy: 0.59375, loss: 1.8851838111877441\n",
      "[sent] train step #2899 accuracy: 0.65625, loss: 1.8851838111877441\n",
      "[spk] train step #2999 accuracy: 0.5, loss: 1.5123066902160645\n",
      "[sent] train step #2999 accuracy: 0.71875, loss: 1.5123066902160645\n",
      "[spk] train step #3099 accuracy: 0.40625, loss: 2.164978265762329\n",
      "[sent] train step #3099 accuracy: 0.59375, loss: 2.164978265762329\n",
      "[spk] train step #3199 accuracy: 0.65625, loss: 1.1651737689971924\n",
      "[sent] train step #3199 accuracy: 0.6875, loss: 1.1651737689971924\n",
      "[spk] train step #3299 accuracy: 0.5, loss: 1.625763177871704\n",
      "[sent] train step #3299 accuracy: 0.625, loss: 1.625763177871704\n",
      "[spk] train step #3399 accuracy: 0.5625, loss: 1.6064045429229736\n",
      "[sent] train step #3399 accuracy: 0.625, loss: 1.6064045429229736\n",
      "epoch #2, final dev accuracy: 0.6461763552867383, 0.6854908714157707\n",
      "saving best model...\n",
      "[spk] train step #3499 accuracy: 0.84375, loss: 0.8023072481155396\n",
      "[sent] train step #3499 accuracy: 0.6875, loss: 0.8023072481155396\n",
      "[spk] train step #3599 accuracy: 0.6875, loss: 1.32639741897583\n",
      "[sent] train step #3599 accuracy: 0.78125, loss: 1.32639741897583\n",
      "[spk] train step #3699 accuracy: 0.75, loss: 1.1133530139923096\n",
      "[sent] train step #3699 accuracy: 0.65625, loss: 1.1133530139923096\n",
      "[spk] train step #3799 accuracy: 0.75, loss: 1.3750102519989014\n",
      "[sent] train step #3799 accuracy: 0.4375, loss: 1.3750102519989014\n",
      "[spk] train step #3899 accuracy: 0.75, loss: 1.051673173904419\n",
      "[sent] train step #3899 accuracy: 0.59375, loss: 1.051673173904419\n",
      "[spk] train step #3999 accuracy: 0.875, loss: 0.7821938991546631\n",
      "[sent] train step #3999 accuracy: 0.625, loss: 0.7821938991546631\n",
      "[spk] train step #4099 accuracy: 0.8125, loss: 0.9475280046463013\n",
      "[sent] train step #4099 accuracy: 0.71875, loss: 0.9475280046463013\n",
      "[spk] train step #4199 accuracy: 0.875, loss: 0.9104453325271606\n",
      "[sent] train step #4199 accuracy: 0.65625, loss: 0.9104453325271606\n",
      "[spk] train step #4299 accuracy: 0.78125, loss: 0.9417157769203186\n",
      "[sent] train step #4299 accuracy: 0.75, loss: 0.9417157769203186\n",
      "[spk] train step #4399 accuracy: 0.6875, loss: 1.2665926218032837\n",
      "[sent] train step #4399 accuracy: 0.6875, loss: 1.2665926218032837\n",
      "[spk] train step #4499 accuracy: 0.78125, loss: 0.9904111623764038\n",
      "[sent] train step #4499 accuracy: 0.65625, loss: 0.9904111623764038\n",
      "[spk] train step #4599 accuracy: 0.6875, loss: 1.2096400260925293\n",
      "[sent] train step #4599 accuracy: 0.59375, loss: 1.2096400260925293\n",
      "epoch #3, final dev accuracy: 0.7225792450716846, 0.7167268705197133\n",
      "saving best model...\n",
      "[spk] train step #4699 accuracy: 0.8125, loss: 1.0652705430984497\n",
      "[sent] train step #4699 accuracy: 0.65625, loss: 1.0652705430984497\n",
      "[spk] train step #4799 accuracy: 0.84375, loss: 1.1724754571914673\n",
      "[sent] train step #4799 accuracy: 0.5625, loss: 1.1724754571914673\n",
      "[spk] train step #4899 accuracy: 0.75, loss: 0.9934426546096802\n",
      "[sent] train step #4899 accuracy: 0.625, loss: 0.9934426546096802\n",
      "[spk] train step #4999 accuracy: 0.8125, loss: 0.686564564704895\n",
      "[sent] train step #4999 accuracy: 0.71875, loss: 0.686564564704895\n",
      "[spk] train step #5099 accuracy: 0.875, loss: 0.5423087477684021\n",
      "[sent] train step #5099 accuracy: 0.78125, loss: 0.5423087477684021\n",
      "[spk] train step #5199 accuracy: 0.78125, loss: 1.0482730865478516\n",
      "[sent] train step #5199 accuracy: 0.6875, loss: 1.0482730865478516\n",
      "[spk] train step #5299 accuracy: 0.75, loss: 1.2234735488891602\n",
      "[sent] train step #5299 accuracy: 0.75, loss: 1.2234735488891602\n",
      "[spk] train step #5399 accuracy: 0.78125, loss: 1.0274518728256226\n",
      "[sent] train step #5399 accuracy: 0.6875, loss: 1.0274518728256226\n",
      "[spk] train step #5499 accuracy: 0.84375, loss: 0.8964610695838928\n",
      "[sent] train step #5499 accuracy: 0.6875, loss: 0.8964610695838928\n",
      "[spk] train step #5599 accuracy: 0.84375, loss: 0.7414079904556274\n",
      "[sent] train step #5599 accuracy: 0.6875, loss: 0.7414079904556274\n",
      "[spk] train step #5699 accuracy: 0.875, loss: 0.6755428910255432\n",
      "[sent] train step #5699 accuracy: 0.65625, loss: 0.6755428910255432\n",
      "epoch #4, final dev accuracy: 0.770154289874552, 0.7440916218637993\n",
      "saving best model...\n",
      "[spk] train step #5799 accuracy: 0.84375, loss: 0.6346726417541504\n",
      "[sent] train step #5799 accuracy: 0.75, loss: 0.6346726417541504\n",
      "[spk] train step #5899 accuracy: 0.90625, loss: 0.742576539516449\n",
      "[sent] train step #5899 accuracy: 0.5625, loss: 0.742576539516449\n",
      "[spk] train step #5999 accuracy: 0.875, loss: 0.6677244901657104\n",
      "[sent] train step #5999 accuracy: 0.5625, loss: 0.6677244901657104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spk] train step #6099 accuracy: 0.84375, loss: 0.4732835292816162\n",
      "[sent] train step #6099 accuracy: 0.78125, loss: 0.4732835292816162\n",
      "[spk] train step #6199 accuracy: 0.9375, loss: 0.4948241710662842\n",
      "[sent] train step #6199 accuracy: 0.71875, loss: 0.4948241710662842\n",
      "[spk] train step #6299 accuracy: 0.96875, loss: 0.3819319009780884\n",
      "[sent] train step #6299 accuracy: 0.75, loss: 0.3819319009780884\n",
      "[spk] train step #6399 accuracy: 0.875, loss: 0.7096893191337585\n",
      "[sent] train step #6399 accuracy: 0.90625, loss: 0.7096893191337585\n",
      "[spk] train step #6499 accuracy: 0.6875, loss: 1.1643742322921753\n",
      "[sent] train step #6499 accuracy: 0.71875, loss: 1.1643742322921753\n",
      "[spk] train step #6599 accuracy: 0.9375, loss: 0.3894484341144562\n",
      "[sent] train step #6599 accuracy: 0.84375, loss: 0.3894484341144562\n",
      "[spk] train step #6699 accuracy: 0.875, loss: 0.6186811923980713\n",
      "[sent] train step #6699 accuracy: 0.71875, loss: 0.6186811923980713\n",
      "[spk] train step #6799 accuracy: 0.875, loss: 0.7141363620758057\n",
      "[sent] train step #6799 accuracy: 0.78125, loss: 0.7141363620758057\n",
      "[spk] train step #6899 accuracy: 0.84375, loss: 0.6832157373428345\n",
      "[sent] train step #6899 accuracy: 0.78125, loss: 0.6832157373428345\n",
      "epoch #5, final dev accuracy: 0.7929407482078853, 0.7812219982078853\n",
      "saving best model...\n",
      "[spk] train step #6999 accuracy: 0.90625, loss: 0.6044840812683105\n",
      "[sent] train step #6999 accuracy: 0.625, loss: 0.6044840812683105\n",
      "[spk] train step #7099 accuracy: 0.875, loss: 0.6320481300354004\n",
      "[sent] train step #7099 accuracy: 0.78125, loss: 0.6320481300354004\n",
      "[spk] train step #7199 accuracy: 0.96875, loss: 0.38111865520477295\n",
      "[sent] train step #7199 accuracy: 0.6875, loss: 0.38111865520477295\n",
      "[spk] train step #7299 accuracy: 0.90625, loss: 0.5058333277702332\n",
      "[sent] train step #7299 accuracy: 0.78125, loss: 0.5058333277702332\n",
      "[spk] train step #7399 accuracy: 0.8125, loss: 0.5444090366363525\n",
      "[sent] train step #7399 accuracy: 0.75, loss: 0.5444090366363525\n",
      "[spk] train step #7499 accuracy: 0.9375, loss: 0.48802900314331055\n",
      "[sent] train step #7499 accuracy: 0.8125, loss: 0.48802900314331055\n",
      "[spk] train step #7599 accuracy: 0.9375, loss: 0.4957594573497772\n",
      "[sent] train step #7599 accuracy: 0.6875, loss: 0.4957594573497772\n",
      "[spk] train step #7699 accuracy: 0.96875, loss: 0.626789927482605\n",
      "[sent] train step #7699 accuracy: 0.625, loss: 0.626789927482605\n",
      "[spk] train step #7799 accuracy: 0.75, loss: 1.0850567817687988\n",
      "[sent] train step #7799 accuracy: 0.8125, loss: 1.0850567817687988\n",
      "[spk] train step #7899 accuracy: 0.90625, loss: 0.43093499541282654\n",
      "[sent] train step #7899 accuracy: 0.8125, loss: 0.43093499541282654\n",
      "[spk] train step #7999 accuracy: 0.875, loss: 0.5635955333709717\n",
      "[sent] train step #7999 accuracy: 0.8125, loss: 0.5635955333709717\n",
      "epoch #6, final dev accuracy: 0.7946558579749105, 0.7868433579749105\n",
      "[spk] train step #8099 accuracy: 0.9375, loss: 0.27598875761032104\n",
      "[sent] train step #8099 accuracy: 0.84375, loss: 0.27598875761032104\n",
      "[spk] train step #8199 accuracy: 0.90625, loss: 0.5260403156280518\n",
      "[sent] train step #8199 accuracy: 0.71875, loss: 0.5260403156280518\n",
      "[spk] train step #8299 accuracy: 0.90625, loss: 0.4749966859817505\n",
      "[sent] train step #8299 accuracy: 0.71875, loss: 0.4749966859817505\n",
      "[spk] train step #8399 accuracy: 0.875, loss: 0.6215500235557556\n",
      "[sent] train step #8399 accuracy: 0.8125, loss: 0.6215500235557556\n",
      "[spk] train step #8499 accuracy: 0.875, loss: 0.3294494152069092\n",
      "[sent] train step #8499 accuracy: 0.8125, loss: 0.3294494152069092\n",
      "[spk] train step #8599 accuracy: 0.9375, loss: 0.5098274350166321\n",
      "[sent] train step #8599 accuracy: 0.6875, loss: 0.5098274350166321\n",
      "[spk] train step #8699 accuracy: 0.875, loss: 0.5768033862113953\n",
      "[sent] train step #8699 accuracy: 0.75, loss: 0.5768033862113953\n",
      "[spk] train step #8799 accuracy: 0.9375, loss: 0.48948702216148376\n",
      "[sent] train step #8799 accuracy: 0.75, loss: 0.48948702216148376\n",
      "[spk] train step #8899 accuracy: 0.90625, loss: 0.5917052030563354\n",
      "[sent] train step #8899 accuracy: 0.75, loss: 0.5917052030563354\n",
      "[spk] train step #8999 accuracy: 0.96875, loss: 0.4342743754386902\n",
      "[sent] train step #8999 accuracy: 0.8125, loss: 0.4342743754386902\n",
      "[spk] train step #9099 accuracy: 0.96875, loss: 0.22215619683265686\n",
      "[sent] train step #9099 accuracy: 0.90625, loss: 0.22215619683265686\n",
      "[spk] train step #9199 accuracy: 0.9375, loss: 0.49713173508644104\n",
      "[sent] train step #9199 accuracy: 0.6875, loss: 0.49713173508644104\n",
      "epoch #7, final dev accuracy: 0.8144041218637993, 0.8035324260752689\n",
      "saving best model...\n",
      "[spk] train step #9299 accuracy: 0.9375, loss: 0.3486041724681854\n",
      "[sent] train step #9299 accuracy: 0.71875, loss: 0.3486041724681854\n",
      "[spk] train step #9399 accuracy: 0.9375, loss: 0.4873538613319397\n",
      "[sent] train step #9399 accuracy: 0.71875, loss: 0.4873538613319397\n",
      "[spk] train step #9499 accuracy: 0.84375, loss: 0.46269237995147705\n",
      "[sent] train step #9499 accuracy: 0.78125, loss: 0.46269237995147705\n",
      "[spk] train step #9599 accuracy: 0.8125, loss: 0.6344835758209229\n",
      "[sent] train step #9599 accuracy: 0.6875, loss: 0.6344835758209229\n",
      "[spk] train step #9699 accuracy: 0.96875, loss: 0.37436020374298096\n",
      "[sent] train step #9699 accuracy: 0.78125, loss: 0.37436020374298096\n",
      "[spk] train step #9799 accuracy: 0.96875, loss: 0.33792179822921753\n",
      "[sent] train step #9799 accuracy: 0.875, loss: 0.33792179822921753\n",
      "[spk] train step #9899 accuracy: 0.9375, loss: 0.3604395389556885\n",
      "[sent] train step #9899 accuracy: 0.71875, loss: 0.3604395389556885\n",
      "[spk] train step #9999 accuracy: 0.96875, loss: 0.6373656988143921\n",
      "[sent] train step #9999 accuracy: 0.65625, loss: 0.6373656988143921\n",
      "[spk] train step #10099 accuracy: 0.90625, loss: 0.4940321743488312\n",
      "[sent] train step #10099 accuracy: 0.90625, loss: 0.4940321743488312\n",
      "[spk] train step #10199 accuracy: 0.96875, loss: 0.36736857891082764\n",
      "[sent] train step #10199 accuracy: 0.84375, loss: 0.36736857891082764\n",
      "[spk] train step #10299 accuracy: 0.9375, loss: 0.6034765243530273\n",
      "[sent] train step #10299 accuracy: 0.625, loss: 0.6034765243530273\n",
      "epoch #8, final dev accuracy: 0.8076696908602151, 0.7977360551075269\n",
      "[spk] train step #10399 accuracy: 0.9375, loss: 0.43889445066452026\n",
      "[sent] train step #10399 accuracy: 0.71875, loss: 0.43889445066452026\n",
      "[spk] train step #10499 accuracy: 1.0, loss: 0.20139512419700623\n",
      "[sent] train step #10499 accuracy: 0.90625, loss: 0.20139512419700623\n",
      "[spk] train step #10599 accuracy: 0.9375, loss: 0.42974090576171875\n",
      "[sent] train step #10599 accuracy: 0.8125, loss: 0.42974090576171875\n",
      "[spk] train step #10699 accuracy: 0.9375, loss: 0.5860060453414917\n",
      "[sent] train step #10699 accuracy: 0.6875, loss: 0.5860060453414917\n",
      "[spk] train step #10799 accuracy: 0.9375, loss: 0.27899402379989624\n",
      "[sent] train step #10799 accuracy: 0.875, loss: 0.27899402379989624\n",
      "[spk] train step #10899 accuracy: 1.0, loss: 0.25043243169784546\n",
      "[sent] train step #10899 accuracy: 0.875, loss: 0.25043243169784546\n",
      "[spk] train step #10999 accuracy: 0.96875, loss: 0.49608051776885986\n",
      "[sent] train step #10999 accuracy: 0.75, loss: 0.49608051776885986\n",
      "[spk] train step #11099 accuracy: 0.96875, loss: 0.4066339433193207\n",
      "[sent] train step #11099 accuracy: 0.75, loss: 0.4066339433193207\n",
      "[spk] train step #11199 accuracy: 0.84375, loss: 0.47327888011932373\n",
      "[sent] train step #11199 accuracy: 0.875, loss: 0.47327888011932373\n",
      "[spk] train step #11299 accuracy: 0.96875, loss: 0.33640143275260925\n",
      "[sent] train step #11299 accuracy: 0.8125, loss: 0.33640143275260925\n",
      "[spk] train step #11399 accuracy: 0.84375, loss: 0.41944485902786255\n",
      "[sent] train step #11399 accuracy: 0.84375, loss: 0.41944485902786255\n",
      "[spk] train step #11499 accuracy: 1.0, loss: 0.36768150329589844\n",
      "[sent] train step #11499 accuracy: 0.78125, loss: 0.36768150329589844\n",
      "epoch #9, final dev accuracy: 0.8207045250896058, 0.8209285394265233\n",
      "saving best model...\n",
      "[spk] train step #11599 accuracy: 0.96875, loss: 0.28342530131340027\n",
      "[sent] train step #11599 accuracy: 0.78125, loss: 0.28342530131340027\n",
      "[spk] train step #11699 accuracy: 1.0, loss: 0.1857597380876541\n",
      "[sent] train step #11699 accuracy: 0.84375, loss: 0.1857597380876541\n",
      "[spk] train step #11799 accuracy: 1.0, loss: 0.34890487790107727\n",
      "[sent] train step #11799 accuracy: 0.6875, loss: 0.34890487790107727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spk] train step #11899 accuracy: 0.96875, loss: 0.23160222172737122\n",
      "[sent] train step #11899 accuracy: 0.8125, loss: 0.23160222172737122\n",
      "[spk] train step #11999 accuracy: 0.9375, loss: 0.24413791298866272\n",
      "[sent] train step #11999 accuracy: 0.9375, loss: 0.24413791298866272\n",
      "[spk] train step #12099 accuracy: 0.9375, loss: 0.2631625831127167\n",
      "[sent] train step #12099 accuracy: 0.84375, loss: 0.2631625831127167\n",
      "[spk] train step #12199 accuracy: 1.0, loss: 0.22426143288612366\n",
      "[sent] train step #12199 accuracy: 0.75, loss: 0.22426143288612366\n",
      "[spk] train step #12299 accuracy: 0.9375, loss: 0.39455196261405945\n",
      "[sent] train step #12299 accuracy: 0.75, loss: 0.39455196261405945\n",
      "[spk] train step #12399 accuracy: 0.96875, loss: 0.3116820156574249\n",
      "[sent] train step #12399 accuracy: 0.78125, loss: 0.3116820156574249\n",
      "[spk] train step #12499 accuracy: 0.96875, loss: 0.251065194606781\n",
      "[sent] train step #12499 accuracy: 0.8125, loss: 0.251065194606781\n",
      "[spk] train step #12599 accuracy: 1.0, loss: 0.22250670194625854\n",
      "[sent] train step #12599 accuracy: 0.8125, loss: 0.22250670194625854\n",
      "epoch #10, final dev accuracy: 0.8226506496415771, 0.824155745967742\n",
      "saving best model...\n",
      "[spk] train step #12699 accuracy: 1.0, loss: 0.23457220196723938\n",
      "[sent] train step #12699 accuracy: 0.875, loss: 0.23457220196723938\n",
      "[spk] train step #12799 accuracy: 0.90625, loss: 0.37614500522613525\n",
      "[sent] train step #12799 accuracy: 0.84375, loss: 0.37614500522613525\n",
      "[spk] train step #12899 accuracy: 1.0, loss: 0.10744399577379227\n",
      "[sent] train step #12899 accuracy: 0.84375, loss: 0.10744399577379227\n",
      "[spk] train step #12999 accuracy: 1.0, loss: 0.23662327229976654\n",
      "[sent] train step #12999 accuracy: 0.8125, loss: 0.23662327229976654\n",
      "[spk] train step #13099 accuracy: 0.96875, loss: 0.20365184545516968\n",
      "[sent] train step #13099 accuracy: 0.90625, loss: 0.20365184545516968\n",
      "[spk] train step #13199 accuracy: 0.96875, loss: 0.29239946603775024\n",
      "[sent] train step #13199 accuracy: 0.84375, loss: 0.29239946603775024\n",
      "[spk] train step #13299 accuracy: 0.9375, loss: 0.23042500019073486\n",
      "[sent] train step #13299 accuracy: 0.8125, loss: 0.23042500019073486\n",
      "[spk] train step #13399 accuracy: 1.0, loss: 0.25542837381362915\n",
      "[sent] train step #13399 accuracy: 0.8125, loss: 0.25542837381362915\n",
      "[spk] train step #13499 accuracy: 0.96875, loss: 0.36166390776634216\n",
      "[sent] train step #13499 accuracy: 0.8125, loss: 0.36166390776634216\n",
      "[spk] train step #13599 accuracy: 1.0, loss: 0.14341676235198975\n",
      "[sent] train step #13599 accuracy: 0.90625, loss: 0.14341676235198975\n",
      "[spk] train step #13699 accuracy: 0.9375, loss: 0.43511635065078735\n",
      "[sent] train step #13699 accuracy: 0.75, loss: 0.43511635065078735\n",
      "[spk] train step #13799 accuracy: 0.9375, loss: 0.4901021718978882\n",
      "[sent] train step #13799 accuracy: 0.78125, loss: 0.4901021718978882\n",
      "epoch #11, final dev accuracy: 0.8233086917562724, 0.8298261088709677\n",
      "[spk] train step #13899 accuracy: 0.9375, loss: 0.3364936113357544\n",
      "[sent] train step #13899 accuracy: 0.8125, loss: 0.3364936113357544\n",
      "[spk] train step #13999 accuracy: 0.96875, loss: 0.30973395705223083\n",
      "[sent] train step #13999 accuracy: 0.75, loss: 0.30973395705223083\n",
      "[spk] train step #14099 accuracy: 0.9375, loss: 0.2987711429595947\n",
      "[sent] train step #14099 accuracy: 0.78125, loss: 0.2987711429595947\n",
      "[spk] train step #14199 accuracy: 1.0, loss: 0.14845934510231018\n",
      "[sent] train step #14199 accuracy: 0.875, loss: 0.14845934510231018\n",
      "[spk] train step #14299 accuracy: 0.96875, loss: 0.18729738891124725\n",
      "[sent] train step #14299 accuracy: 0.8125, loss: 0.18729738891124725\n",
      "[spk] train step #14399 accuracy: 1.0, loss: 0.18585160374641418\n",
      "[sent] train step #14399 accuracy: 0.84375, loss: 0.18585160374641418\n",
      "[spk] train step #14499 accuracy: 0.96875, loss: 0.27907490730285645\n",
      "[sent] train step #14499 accuracy: 0.84375, loss: 0.27907490730285645\n",
      "[spk] train step #14599 accuracy: 0.875, loss: 0.4664866030216217\n",
      "[sent] train step #14599 accuracy: 0.75, loss: 0.4664866030216217\n",
      "[spk] train step #14699 accuracy: 0.96875, loss: 0.41590455174446106\n",
      "[sent] train step #14699 accuracy: 0.75, loss: 0.41590455174446106\n",
      "[spk] train step #14799 accuracy: 0.96875, loss: 0.09893550723791122\n",
      "[sent] train step #14799 accuracy: 0.96875, loss: 0.09893550723791122\n",
      "[spk] train step #14899 accuracy: 1.0, loss: 0.27384674549102783\n",
      "[sent] train step #14899 accuracy: 0.8125, loss: 0.27384674549102783\n",
      "epoch #12, final dev accuracy: 0.8263538866487455, 0.8493503584229392\n",
      "saving best model...\n",
      "[spk] train step #14999 accuracy: 1.0, loss: 0.17775018513202667\n",
      "[sent] train step #14999 accuracy: 0.78125, loss: 0.17775018513202667\n",
      "[spk] train step #15099 accuracy: 0.96875, loss: 0.40786072611808777\n",
      "[sent] train step #15099 accuracy: 0.6875, loss: 0.40786072611808777\n",
      "[spk] train step #15199 accuracy: 0.90625, loss: 0.3004167079925537\n",
      "[sent] train step #15199 accuracy: 0.90625, loss: 0.3004167079925537\n",
      "[spk] train step #15299 accuracy: 1.0, loss: 0.2639983594417572\n",
      "[sent] train step #15299 accuracy: 0.78125, loss: 0.2639983594417572\n",
      "[spk] train step #15399 accuracy: 0.9375, loss: 0.22711770236492157\n",
      "[sent] train step #15399 accuracy: 0.8125, loss: 0.22711770236492157\n",
      "[spk] train step #15499 accuracy: 0.96875, loss: 0.2692737579345703\n",
      "[sent] train step #15499 accuracy: 0.8125, loss: 0.2692737579345703\n",
      "[spk] train step #15599 accuracy: 0.96875, loss: 0.19792373478412628\n",
      "[sent] train step #15599 accuracy: 0.8125, loss: 0.19792373478412628\n",
      "[spk] train step #15699 accuracy: 0.90625, loss: 0.28432512283325195\n",
      "[sent] train step #15699 accuracy: 0.90625, loss: 0.28432512283325195\n",
      "[spk] train step #15799 accuracy: 1.0, loss: 0.20823165774345398\n",
      "[sent] train step #15799 accuracy: 0.84375, loss: 0.20823165774345398\n",
      "[spk] train step #15899 accuracy: 0.96875, loss: 0.5862376093864441\n",
      "[sent] train step #15899 accuracy: 0.84375, loss: 0.5862376093864441\n",
      "[spk] train step #15999 accuracy: 1.0, loss: 0.15606337785720825\n",
      "[sent] train step #15999 accuracy: 0.875, loss: 0.15606337785720825\n",
      "[spk] train step #16099 accuracy: 1.0, loss: 0.21781094372272491\n",
      "[sent] train step #16099 accuracy: 0.78125, loss: 0.21781094372272491\n",
      "epoch #13, final dev accuracy: 0.8391717069892474, 0.8474182347670252\n",
      "[spk] train step #16199 accuracy: 0.96875, loss: 0.2265455573797226\n",
      "[sent] train step #16199 accuracy: 0.84375, loss: 0.2265455573797226\n",
      "[spk] train step #16299 accuracy: 1.0, loss: 0.18197350203990936\n",
      "[sent] train step #16299 accuracy: 0.875, loss: 0.18197350203990936\n",
      "[spk] train step #16399 accuracy: 0.9375, loss: 0.3776959776878357\n",
      "[sent] train step #16399 accuracy: 0.78125, loss: 0.3776959776878357\n",
      "[spk] train step #16499 accuracy: 1.0, loss: 0.17135381698608398\n",
      "[sent] train step #16499 accuracy: 0.75, loss: 0.17135381698608398\n",
      "[spk] train step #16599 accuracy: 1.0, loss: 0.15060746669769287\n",
      "[sent] train step #16599 accuracy: 0.90625, loss: 0.15060746669769287\n",
      "[spk] train step #16699 accuracy: 0.96875, loss: 0.19300489127635956\n",
      "[sent] train step #16699 accuracy: 0.875, loss: 0.19300489127635956\n",
      "[spk] train step #16799 accuracy: 0.90625, loss: 0.47521644830703735\n",
      "[sent] train step #16799 accuracy: 0.875, loss: 0.47521644830703735\n",
      "[spk] train step #16899 accuracy: 0.9375, loss: 0.36695408821105957\n",
      "[sent] train step #16899 accuracy: 0.78125, loss: 0.36695408821105957\n",
      "[spk] train step #16999 accuracy: 0.90625, loss: 0.2696090638637543\n",
      "[sent] train step #16999 accuracy: 0.84375, loss: 0.2696090638637543\n",
      "[spk] train step #17099 accuracy: 0.96875, loss: 0.2741371989250183\n",
      "[sent] train step #17099 accuracy: 0.8125, loss: 0.2741371989250183\n",
      "[spk] train step #17199 accuracy: 0.9375, loss: 0.36372900009155273\n",
      "[sent] train step #17199 accuracy: 0.875, loss: 0.36372900009155273\n",
      "epoch #14, final dev accuracy: 0.83329833109319, 0.8543696796594982\n",
      "test accuracy: 0.8304701500896058, 0.8569458445340502\n"
     ]
    }
   ],
   "source": [
    "hk.mtl_train(mtl_config, model=mtl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_model.load(mtl_config['output_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechModel (\n",
       "  (conv1): Conv2d(1, 64, kernel_size=[20, 8], stride=(1, 1))\n",
       "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=[10, 4], stride=(1, 1))\n",
       "  (pool2): MaxPool2d (size=(1, 1), stride=(1, 1), dilation=(1, 1))\n",
       "  (bottleneck): Linear (26624 -> 256)\n",
       "  (output): Linear (256 -> 2000)\n",
       "  (dropout): Dropout (p = 0.5)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_spks = list(sv_spks)\n",
    "enroll_config = si_config.copy()\n",
    "enroll_model = si_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sv_spks)\n",
    "enroll_spks = sv_spks[:5]\n",
    "test_spks = sv_spks[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_uttrs = pd.DataFrame()\n",
    "dev_uttrs = pd.DataFrame()\n",
    "enroll_pts = 0.3\n",
    "\n",
    "# splits enroll and dev\n",
    "for spk in enroll_spks:\n",
    "    spk_df = sv_df[sv_df.spk == spk]\n",
    "    assert(len(spk_df) != 0)\n",
    "    for sent in main_sents:\n",
    "        sent_uttrs = spk_df[spk_df.sent == sent]\n",
    "        if len(sent_uttrs) < 2:\n",
    "            enls = sent_uttrs.sample(n=1)\n",
    "        else:\n",
    "            enls = sent_uttrs.sample(frac=enroll_pts)\n",
    "        devs = sent_uttrs.drop(index=enls.index)\n",
    "        enroll_uttrs = pd.concat([enls, enroll_uttrs])\n",
    "        dev_uttrs = pd.concat([devs, dev_uttrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolled speakers: ['bdee441c', 'c120e80e', '28ce0c58', 'd0faf7e4', 'c1d39ce8']\n",
      "number of enroll uttrs per spk: 44\n"
     ]
    }
   ],
   "source": [
    "print(\"enrolled speakers: {}\".format(enroll_spks))\n",
    "print(\"number of enroll uttrs per spk: %d\"%(len(enroll_uttrs)/len(enroll_spks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./manifests/commands/enroll/enroll_command_bdee441c_manifest.csv was written\n",
      "./manifests/commands/enroll/enroll_command_c120e80e_manifest.csv was written\n",
      "./manifests/commands/enroll/enroll_command_28ce0c58_manifest.csv was written\n",
      "./manifests/commands/enroll/enroll_command_d0faf7e4_manifest.csv was written\n",
      "./manifests/commands/enroll/enroll_command_c1d39ce8_manifest.csv was written\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# delete existing files\n",
    "for file in os.listdir(manifest_dir):\n",
    "    file_path = os.path.join(manifest_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "        \n",
    "manifest_dir = \"./manifests/commands/enroll/\"\n",
    "for spk in enroll_spks:\n",
    "    samples = []\n",
    "    save_path = os.path.join(manifest_dir,'enroll_{}_{}_manifest.csv'.format(\"command\", spk))\n",
    "    with open(save_path, 'w') as f:\n",
    "        for index, row in enroll_uttrs[enroll_uttrs.spk == spk].iterrows():\n",
    "            file_path = os.path.join(data_dir, row.sent, row.file)\n",
    "            sent_label = sent_labels.index(row.sent)\n",
    "            sample = ','.join([file_path, str(sent_label)])\n",
    "            samples.append(sample)\n",
    "        writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "        writer.writerow(samples)\n",
    "        print(\"{} was written\".format(save_path))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechModel (\n",
       "  (conv1): Conv2d(1, 64, kernel_size=[20, 8], stride=(1, 1))\n",
       "  (pool1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=[10, 4], stride=(1, 1))\n",
       "  (pool2): MaxPool2d (size=(1, 1), stride=(1, 1), dilation=(1, 1))\n",
       "  (bottleneck): Linear (26624 -> 256)\n",
       "  (output): Linear (256 -> 2000)\n",
       "  (dropout): Dropout (p = 0.5)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enroll_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# enrolling speakers\n",
    "# generating speaker models\n",
    "\n",
    "manifest_dir = \"./manifests/commands/enroll/\"\n",
    "word_spk_models = dict()\n",
    "uni_spk_models = dict()\n",
    "for spk in enroll_spks:\n",
    "    manifest_path = os.path.join(manifest_dir, 'enroll_{}_{}_manifest.csv'.format(\"command\", spk))\n",
    "    enroll_config['test_manifest'] =  manifest_path\n",
    "    word_spk_models[spk] = hk.enroll(enroll_config, nb_sent=len(sent_labels), model=enroll_model)\n",
    "    uni_spk_models[spk] = np.mean(word_spk_models[spk], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bdee441c, c120e80e: 0.25\n",
      "bdee441c, 28ce0c58: -0.00\n",
      "bdee441c, d0faf7e4: 0.55\n",
      "bdee441c, c1d39ce8: 0.18\n",
      "c120e80e, bdee441c: 0.25\n",
      "c120e80e, 28ce0c58: 0.11\n",
      "c120e80e, d0faf7e4: 0.45\n",
      "c120e80e, c1d39ce8: 0.42\n",
      "28ce0c58, bdee441c: -0.00\n",
      "28ce0c58, c120e80e: 0.11\n",
      "28ce0c58, d0faf7e4: 0.07\n",
      "28ce0c58, c1d39ce8: -0.11\n",
      "d0faf7e4, bdee441c: 0.55\n",
      "d0faf7e4, c120e80e: 0.45\n",
      "d0faf7e4, 28ce0c58: 0.07\n",
      "d0faf7e4, c1d39ce8: 0.43\n",
      "c1d39ce8, bdee441c: 0.18\n",
      "c1d39ce8, c120e80e: 0.42\n",
      "c1d39ce8, 28ce0c58: -0.11\n",
      "c1d39ce8, d0faf7e4: 0.43\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for spk in enroll_spks:\n",
    "    target = uni_spk_models[spk]\n",
    "    for ref in enroll_spks:\n",
    "        if ref == spk: continue\n",
    "        score = 1-cosine(uni_spk_models[ref], target)\n",
    "        print(\"{}, {}: {:.2f}\".format(spk, ref, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "spk_model = word_spk_models[spk]\n",
    "word_models = [x for x in np.split(spk_model, spk_model.shape[0], axis=0)]\n",
    "model_scores = []\n",
    "for u, v in itertools.combinations(word_models, 2):\n",
    "    score = 1-cosine(u, v)\n",
    "    model_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  6.,  11.,  21.,  35.,  34.,  37.,  31.,  10.,   3.,   2.]),\n",
       " array([-0.0273716 ,  0.0509064 ,  0.1291844 ,  0.20746239,  0.28574039,\n",
       "         0.36401839,  0.44229639,  0.52057439,  0.59885239,  0.67713038,\n",
       "         0.75540838]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVzLC327lYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff631584550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"word-level model variability in a speaker\")\n",
    "plt.hist(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_model = si_model\n",
    "sv_config = si_config.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def spk_verify(spk_model, test_in, sent_label=None, who=False):\n",
    "    best_score = -2\n",
    "    pred_spk = 'Unknown'\n",
    "    is_unknown = True\n",
    "    \n",
    "    \n",
    "    for spk in spk_model.keys():\n",
    "        if sent_label is not None:\n",
    "            signature = spk_model[spk][sent_label]\n",
    "            signature_uni = np.mean(spk_model[spk],0)\n",
    "            score = max(1-cosine(test_in, signature), 1-cosine(test_in,signature_uni))\n",
    "        else:\n",
    "            signature_uni = spk_model[spk]\n",
    "            score = 1-cosine(test_in, signature_uni)\n",
    "            \n",
    "        if score > best_score:\n",
    "            best_score = score     \n",
    "            pred_spk = spk\n",
    "    if who:\n",
    "        return pred_spk, best_score\n",
    "    else:\n",
    "        return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def roc_auc_eer(dists, labels):\n",
    "    \"\"\"\n",
    "        dists: 1D-Array, [samples]\n",
    "        labels: 1D-Array, [samples]\n",
    "    \"\"\"\n",
    "    fpr, tpr, thres = roc_curve(labels, dists, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fnr = 1 - tpr\n",
    "    eer_ths = thres[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    roc_bundle = {'fpr':fpr, 'tpr':tpr, 'thres':thres}\n",
    "    np.save(open('roc_data.npy', 'wb'), roc_bundle)\n",
    "\n",
    "    return roc_auc, eer, eer_ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dev uttrs: 447\n",
      "number of test uttrs: 447\n"
     ]
    }
   ],
   "source": [
    "test_uttrs = sv_df[(sv_df.spk.isin(test_spks)) & (sv_df.sent.isin(main_sents))].sample(n=len(dev_uttrs))\n",
    "print(\"number of dev uttrs: %d\"%len(dev_uttrs))\n",
    "print(\"number of test uttrs: %d\"%len(test_uttrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "bdee441c c1d39ce8\n",
      "bdee441c c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "28ce0c58 c1d39ce8\n",
      "28ce0c58 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "d0faf7e4 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "c1d39ce8 c1d39ce8\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "d0faf7e4 d0faf7e4\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "c1d39ce8 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "28ce0c58 28ce0c58\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "bdee441c c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "c120e80e c120e80e\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n",
      "bdee441c bdee441c\n"
     ]
    }
   ],
   "source": [
    "# si accuracy based on sv protocol\n",
    "uni_pos_scores = []\n",
    "uni_neg_scores = []\n",
    "speaker_model = uni_spk_models\n",
    "for i, row in dev_uttrs.iterrows():\n",
    "    audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "    test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "    word_label = sent_labels.index(row.sent)\n",
    "    pred_spk, score = spk_verify(uni_spk_models, test_in, None, who = True)\n",
    "    print(pred_spk, row.spk)\n",
    "#     score = spk_verify(word_spk_models, test_in, word_label)\n",
    "    uni_pos_scores.append(score)\n",
    "    \n",
    "# for i, row in test_uttrs.iterrows():\n",
    "#     audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "# #     test_in = np.concatenate((hk.embed(sv_config, si_model, audio_path), \n",
    "# #                          hk.embed(sv_config, kws_model, audio_path)),axis=1)\n",
    "#     test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "#     word_label = sent_labels.index(row.sent)\n",
    "#     score = spk_verify(uni_spk_models, test_in)\n",
    "#     uni_neg_scores.append(score)\n",
    "\n",
    "# dists = np.concatenate((uni_pos_scores, uni_neg_scores), 0)\n",
    "# labels = np.concatenate((np.ones_like(uni_pos_scores), np.zeros_like(uni_neg_scores)), 0)\n",
    "# auc_, eer_, uni_eer_ths = roc_auc_eer(dists, labels)\n",
    "# print(\"auc: {}, eer: {}\".format(auc_, eer_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.9754915944727215, eer: 0.07829977628635347\n"
     ]
    }
   ],
   "source": [
    "uni_pos_scores = []\n",
    "uni_neg_scores = []\n",
    "speaker_model = uni_spk_models\n",
    "for i, row in dev_uttrs.iterrows():\n",
    "    audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "    test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "    word_label = sent_labels.index(row.sent)\n",
    "    score = spk_verify(uni_spk_models, test_in, None)\n",
    "#     score = spk_verify(word_spk_models, test_in, word_label)\n",
    "    uni_pos_scores.append(score)\n",
    "    \n",
    "for i, row in test_uttrs.iterrows():\n",
    "    audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "#     test_in = np.concatenate((hk.embed(sv_config, si_model, audio_path), \n",
    "#                          hk.embed(sv_config, kws_model, audio_path)),axis=1)\n",
    "    test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "    word_label = sent_labels.index(row.sent)\n",
    "    score = spk_verify(uni_spk_models, test_in)\n",
    "    uni_neg_scores.append(score)\n",
    "\n",
    "dists = np.concatenate((uni_pos_scores, uni_neg_scores), 0)\n",
    "labels = np.concatenate((np.ones_like(uni_pos_scores), np.zeros_like(uni_neg_scores)), 0)\n",
    "auc_, eer_, uni_eer_ths = roc_auc_eer(dists, labels)\n",
    "print(\"auc: {}, eer: {}\".format(auc_, eer_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,  10.,  24.,  33.,  36.,  40.,  35.,  21.,   9.,   6.]),\n",
       " array([-0.0940329 , -0.02653204,  0.04096881,  0.10846966,  0.17597052,\n",
       "         0.24347137,  0.31097223,  0.37847308,  0.44597394,  0.51347479,\n",
       "         0.58097565]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE7yZWaac4M3MMuUEb2aWKSd4M7NM/R/gh8rqNkupuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6319313c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subplot(121)\n",
    "plt.xlim([0, 1.0])\n",
    "plt.title(\"uni, positive\")\n",
    "plt.hist(uni_pos_scores)\n",
    "subplot(122)\n",
    "plt.xlim([0, 1.0])\n",
    "plt.title(\"uni, negative\")\n",
    "plt.hist(uni_neg_scores, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.967292524005487, eer: 0.09259259259259259\n"
     ]
    }
   ],
   "source": [
    "word_pos_scores = []\n",
    "word_neg_scores = []\n",
    "speaker_model = uni_spk_models\n",
    "for i, row in dev_uttrs.iterrows():\n",
    "    audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "    test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "    word_label = sent_labels.index(row.sent)\n",
    "    score = spk_verify(word_spk_models, test_in, word_label)\n",
    "    word_pos_scores.append(score)\n",
    "    \n",
    "for i, row in test_uttrs.iterrows():\n",
    "    audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "    test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "    word_label = sent_labels.index(row.sent)\n",
    "    score = spk_verify(word_spk_models, test_in, word_label)\n",
    "    word_neg_scores.append(score)\n",
    "\n",
    "dists = np.concatenate((word_pos_scores, word_neg_scores), 0)\n",
    "labels = np.concatenate((np.ones_like(word_pos_scores), np.zeros_like(word_neg_scores)), 0)\n",
    "auc_, eer_, word_eer_ths = roc_auc_eer(dists, labels)\n",
    "print(\"auc: {}, eer: {}\".format(auc_, eer_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   7.,  16.,  32.,  31.,  45.,  40.,  25.,  14.,   4.]),\n",
       " array([-0.01464383,  0.05175966,  0.11816315,  0.18456664,  0.25097013,\n",
       "         0.31737362,  0.38377711,  0.45018061,  0.5165841 ,  0.58298759,\n",
       "         0.64939108]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff491ca92b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subplot(121)\n",
    "plt.xlim([0, 1.0])\n",
    "plt.title(\"word, positive\")\n",
    "plt.hist(word_pos_scores)\n",
    "subplot(122)\n",
    "plt.xlim([0, 1.0])\n",
    "plt.title(\"word, negative\") \n",
    "plt.hist(word_neg_scores, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni_fr: 23, uni_fa: 22\n"
     ]
    }
   ],
   "source": [
    "uni_fr_samples = dev_uttrs[(np.array(uni_pos_scores) < uni_eer_ths)]\n",
    "uni_fa_samples = test_uttrs[(np.array(uni_neg_scores) > uni_eer_ths)]\n",
    "uni_bad_samples = pd.concat([uni_fr_samples, uni_fa_samples])\n",
    "print(\"uni_fr: %d, uni_fa: %d\"%(len(uni_fr_samples), len(uni_fa_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_fr: 21, word_fa: 19\n"
     ]
    }
   ],
   "source": [
    "word_fr_samples = dev_uttrs[(np.array(word_pos_scores) < word_eer_ths)]\n",
    "word_fa_samples = test_uttrs[(np.array(word_neg_scores) > word_eer_ths)]\n",
    "word_bad_samples = pd.concat([word_fr_samples, word_fa_samples])\n",
    "print(\"word_fr: %d, word_fa: %d\"%(len(word_fr_samples), len(word_fa_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared bad examples: 20/40\n"
     ]
    }
   ],
   "source": [
    "# overlapped bad examles\n",
    "overlap_bad_examples = word_bad_samples[word_bad_samples.index.isin(uni_bad_samples.index)]\n",
    "print(\"shared bad examples: %d/%d\"%(len(overlap_bad_examples), len(word_bad_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolled_spks:['bdee441c', 'c120e80e', '28ce0c58', 'd0faf7e4', 'c1d39ce8']\n",
      "c1d39ce8 : 0.76\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.70\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.67\n",
      "c1d39ce8 : 0.70\n",
      "c1d39ce8 : 0.67\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.21\n",
      "c1d39ce8 : 0.80\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.64\n",
      "c1d39ce8 : 0.70\n",
      "c1d39ce8 : 0.35\n",
      "c1d39ce8 : 0.70\n",
      "c1d39ce8 : 0.54\n",
      "c1d39ce8 : 0.64\n",
      "c1d39ce8 : 0.41\n",
      "c1d39ce8 : 0.38\n",
      "c1d39ce8 : 0.38\n",
      "c1d39ce8 : 0.79\n",
      "c1d39ce8 : 0.41\n",
      "c1d39ce8 : 0.46\n",
      "c1d39ce8 : 0.37\n",
      "c1d39ce8 : 0.46\n",
      "c1d39ce8 : 0.46\n",
      "c1d39ce8 : 0.58\n",
      "c1d39ce8 : 0.43\n",
      "c1d39ce8 : 0.58\n",
      "c1d39ce8 : 0.80\n",
      "c1d39ce8 : 0.60\n",
      "c1d39ce8 : 0.79\n",
      "c1d39ce8 : 0.82\n",
      "c1d39ce8 : 0.82\n",
      "c1d39ce8 : 0.80\n",
      "c1d39ce8 : 0.84\n",
      "c1d39ce8 : 0.53\n",
      "c1d39ce8 : 0.45\n",
      "c1d39ce8 : 0.53\n",
      "c1d39ce8 : 0.59\n",
      "c1d39ce8 : 0.60\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.60\n",
      "c1d39ce8 : 0.65\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.83\n",
      "c1d39ce8 : 0.65\n",
      "c1d39ce8 : 0.83\n",
      "c1d39ce8 : 0.83\n",
      "c1d39ce8 : 0.58\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.76\n",
      "c1d39ce8 : 0.81\n",
      "c1d39ce8 : 0.72\n",
      "c1d39ce8 : 0.76\n",
      "c1d39ce8 : 0.71\n",
      "c1d39ce8 : 0.72\n",
      "c1d39ce8 : 0.78\n",
      "c1d39ce8 : 0.52\n",
      "c1d39ce8 : 0.70\n",
      "c1d39ce8 : 0.63\n",
      "c1d39ce8 : 0.78\n",
      "c1d39ce8 : 0.63\n",
      "c1d39ce8 : 0.82\n",
      "c1d39ce8 : 0.78\n",
      "c1d39ce8 : 0.67\n",
      "c1d39ce8 : 0.67\n",
      "c1d39ce8 : 0.68\n",
      "c1d39ce8 : 0.68\n",
      "c1d39ce8 : 0.67\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.29\n",
      "c1d39ce8 : 0.29\n",
      "c1d39ce8 : 0.40\n",
      "c1d39ce8 : 0.40\n",
      "c1d39ce8 : 0.69\n",
      "c1d39ce8 : 0.73\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.40\n",
      "c1d39ce8 : 0.85\n",
      "c1d39ce8 : 0.72\n",
      "c1d39ce8 : 0.54\n",
      "c1d39ce8 : 0.72\n",
      "c1d39ce8 : 0.45\n",
      "c1d39ce8 : 0.54\n",
      "c1d39ce8 : 0.71\n",
      "c1d39ce8 : 0.73\n",
      "c1d39ce8 : 0.71\n",
      "c1d39ce8 : 0.59\n",
      "c1d39ce8 : 0.63\n",
      "c1d39ce8 : 0.70\n",
      "c1d39ce8 : 0.63\n",
      "c1d39ce8 : 0.85\n",
      "c1d39ce8 : 0.60\n",
      "c1d39ce8 : 0.43\n",
      "c1d39ce8 : 0.43\n",
      "c1d39ce8 : 0.82\n",
      "c1d39ce8 : 0.60\n",
      "c1d39ce8 : 0.49\n",
      "c1d39ce8 : 0.49\n",
      "c1d39ce8 : 0.55\n",
      "c1d39ce8 : 0.73\n",
      "c1d39ce8 : 0.73\n",
      "c1d39ce8 : 0.46\n",
      "c1d39ce8 : 0.86\n",
      "c1d39ce8 : 0.86\n",
      "c1d39ce8 : 0.76\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.76\n",
      "c1d39ce8 : 0.19\n",
      "c1d39ce8 : 0.51\n",
      "c1d39ce8 : 0.46\n",
      "c1d39ce8 : 0.51\n",
      "c1d39ce8 : 0.34\n",
      "c1d39ce8 : 0.19\n",
      "c1d39ce8 : 0.49\n",
      "c1d39ce8 : 0.71\n",
      "c1d39ce8 : 0.58\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.72\n",
      "c1d39ce8 : 0.75\n",
      "c1d39ce8 : 0.58\n",
      "c1d39ce8 : 0.81\n",
      "c1d39ce8 : 0.81\n",
      "c1d39ce8 : 0.82\n",
      "c1d39ce8 : 0.51\n",
      "c1d39ce8 : 0.82\n",
      "c1d39ce8 : 0.81\n",
      "c1d39ce8 : 0.25\n",
      "d0faf7e4 : 0.55\n",
      "d0faf7e4 : 0.65\n",
      "d0faf7e4 : 0.55\n",
      "d0faf7e4 : 0.77\n",
      "d0faf7e4 : 0.68\n",
      "d0faf7e4 : 0.38\n",
      "d0faf7e4 : 0.45\n",
      "d0faf7e4 : 0.72\n",
      "d0faf7e4 : 0.70\n",
      "d0faf7e4 : 0.76\n",
      "d0faf7e4 : 0.62\n",
      "d0faf7e4 : 0.69\n",
      "d0faf7e4 : 0.66\n",
      "d0faf7e4 : 0.68\n",
      "d0faf7e4 : 0.52\n",
      "d0faf7e4 : 0.70\n",
      "d0faf7e4 : 0.68\n",
      "d0faf7e4 : 0.58\n",
      "d0faf7e4 : 0.52\n",
      "d0faf7e4 : 0.75\n",
      "d0faf7e4 : 0.62\n",
      "d0faf7e4 : 0.68\n",
      "d0faf7e4 : 0.65\n",
      "d0faf7e4 : 0.65\n",
      "d0faf7e4 : 0.68\n",
      "d0faf7e4 : 0.79\n",
      "d0faf7e4 : 0.46\n",
      "d0faf7e4 : 0.70\n",
      "d0faf7e4 : 0.66\n",
      "d0faf7e4 : 0.72\n",
      "d0faf7e4 : 0.72\n",
      "d0faf7e4 : 0.62\n",
      "d0faf7e4 : 0.77\n",
      "d0faf7e4 : 0.71\n",
      "d0faf7e4 : 0.75\n",
      "d0faf7e4 : 0.76\n",
      "d0faf7e4 : 0.78\n",
      "d0faf7e4 : 0.62\n",
      "d0faf7e4 : 0.79\n",
      "d0faf7e4 : 0.73\n",
      "d0faf7e4 : 0.66\n",
      "d0faf7e4 : 0.69\n",
      "d0faf7e4 : 0.69\n",
      "d0faf7e4 : 0.47\n",
      "d0faf7e4 : 0.76\n",
      "d0faf7e4 : 0.68\n",
      "d0faf7e4 : 0.69\n",
      "d0faf7e4 : 0.66\n",
      "d0faf7e4 : 0.79\n",
      "d0faf7e4 : 0.78\n",
      "d0faf7e4 : 0.73\n",
      "d0faf7e4 : 0.58\n",
      "d0faf7e4 : 0.62\n",
      "d0faf7e4 : 0.74\n",
      "d0faf7e4 : 0.63\n",
      "d0faf7e4 : 0.76\n",
      "d0faf7e4 : 0.81\n",
      "d0faf7e4 : 0.74\n",
      "d0faf7e4 : 0.64\n",
      "d0faf7e4 : 0.69\n",
      "d0faf7e4 : 0.60\n",
      "d0faf7e4 : 0.76\n",
      "d0faf7e4 : 0.78\n",
      "d0faf7e4 : 0.71\n",
      "d0faf7e4 : 0.73\n",
      "d0faf7e4 : 0.76\n",
      "d0faf7e4 : 0.73\n",
      "28ce0c58 : 0.75\n",
      "28ce0c58 : 0.55\n",
      "28ce0c58 : 0.42\n",
      "28ce0c58 : 0.48\n",
      "28ce0c58 : 0.62\n",
      "28ce0c58 : 0.63\n",
      "28ce0c58 : 0.52\n",
      "28ce0c58 : 0.58\n",
      "28ce0c58 : 0.67\n",
      "28ce0c58 : 0.65\n",
      "28ce0c58 : 0.58\n",
      "28ce0c58 : 0.63\n",
      "28ce0c58 : 0.69\n",
      "28ce0c58 : 0.44\n",
      "28ce0c58 : 0.43\n",
      "28ce0c58 : 0.56\n",
      "28ce0c58 : 0.73\n",
      "28ce0c58 : 0.65\n",
      "28ce0c58 : 0.63\n",
      "28ce0c58 : 0.78\n",
      "28ce0c58 : 0.57\n",
      "28ce0c58 : 0.57\n",
      "28ce0c58 : 0.73\n",
      "28ce0c58 : 0.62\n",
      "28ce0c58 : 0.42\n",
      "28ce0c58 : 0.67\n",
      "28ce0c58 : 0.46\n",
      "28ce0c58 : 0.46\n",
      "28ce0c58 : 0.60\n",
      "28ce0c58 : 0.40\n",
      "28ce0c58 : 0.36\n",
      "28ce0c58 : 0.65\n",
      "28ce0c58 : 0.61\n",
      "28ce0c58 : 0.55\n",
      "28ce0c58 : 0.67\n",
      "28ce0c58 : 0.75\n",
      "28ce0c58 : 0.75\n",
      "28ce0c58 : 0.56\n",
      "28ce0c58 : 0.60\n",
      "28ce0c58 : 0.50\n",
      "28ce0c58 : 0.50\n",
      "28ce0c58 : 0.60\n",
      "28ce0c58 : 0.74\n",
      "28ce0c58 : 0.75\n",
      "28ce0c58 : 0.59\n",
      "28ce0c58 : 0.64\n",
      "28ce0c58 : 0.75\n",
      "28ce0c58 : 0.63\n",
      "28ce0c58 : 0.67\n",
      "28ce0c58 : 0.45\n",
      "28ce0c58 : 0.53\n",
      "28ce0c58 : 0.59\n",
      "28ce0c58 : 0.65\n",
      "28ce0c58 : 0.60\n",
      "28ce0c58 : 0.66\n",
      "28ce0c58 : 0.84\n",
      "28ce0c58 : 0.54\n",
      "28ce0c58 : 0.67\n",
      "28ce0c58 : 0.69\n",
      "28ce0c58 : 0.69\n",
      "28ce0c58 : 0.59\n",
      "28ce0c58 : 0.66\n",
      "28ce0c58 : 0.58\n",
      "28ce0c58 : 0.76\n",
      "28ce0c58 : 0.66\n",
      "28ce0c58 : 0.67\n",
      "28ce0c58 : 0.04\n",
      "28ce0c58 : 0.45\n",
      "28ce0c58 : 0.60\n",
      "28ce0c58 : 0.68\n",
      "28ce0c58 : 0.60\n",
      "28ce0c58 : 0.64\n",
      "28ce0c58 : 0.58\n",
      "28ce0c58 : 0.66\n",
      "28ce0c58 : 0.68\n",
      "28ce0c58 : 0.49\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.61\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.51\n",
      "c120e80e : 0.61\n",
      "c120e80e : 0.58\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.56\n",
      "c120e80e : 0.61\n",
      "c120e80e : 0.61\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.65\n",
      "c120e80e : 0.53\n",
      "c120e80e : 0.58\n",
      "c120e80e : 0.60\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.46\n",
      "c120e80e : 0.58\n",
      "c120e80e : 0.60\n",
      "c120e80e : 0.58\n",
      "c120e80e : 0.70\n",
      "c120e80e : 0.61\n",
      "c120e80e : 0.64\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.64\n",
      "c120e80e : 0.58\n",
      "c120e80e : 0.74\n",
      "c120e80e : 0.60\n",
      "c120e80e : 0.65\n",
      "c120e80e : 0.70\n",
      "c120e80e : 0.64\n",
      "c120e80e : 0.57\n",
      "c120e80e : 0.70\n",
      "c120e80e : 0.70\n",
      "c120e80e : 0.72\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.72\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.72\n",
      "c120e80e : 0.67\n",
      "c120e80e : 0.71\n",
      "c120e80e : 0.67\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.71\n",
      "c120e80e : 0.71\n",
      "c120e80e : 0.71\n",
      "c120e80e : 0.65\n",
      "c120e80e : 0.46\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.65\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.75\n",
      "c120e80e : 0.57\n",
      "c120e80e : 0.62\n",
      "c120e80e : 0.66\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.60\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.67\n",
      "c120e80e : 0.70\n",
      "c120e80e : 0.67\n",
      "c120e80e : 0.76\n",
      "c120e80e : 0.51\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.51\n",
      "c120e80e : 0.73\n",
      "c120e80e : 0.59\n",
      "c120e80e : 0.54\n",
      "c120e80e : 0.76\n",
      "c120e80e : 0.73\n",
      "c120e80e : 0.72\n",
      "c120e80e : 0.75\n",
      "c120e80e : 0.76\n",
      "c120e80e : 0.67\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.67\n",
      "c120e80e : 0.75\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.68\n",
      "c120e80e : 0.68\n",
      "c120e80e : 0.73\n",
      "c120e80e : 0.55\n",
      "c120e80e : 0.68\n",
      "c120e80e : 0.63\n",
      "c120e80e : 0.68\n",
      "c120e80e : 0.62\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.68\n",
      "c120e80e : 0.61\n",
      "c120e80e : 0.73\n",
      "c120e80e : 0.44\n",
      "c120e80e : 0.44\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.44\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.69\n",
      "c120e80e : 0.57\n",
      "bdee441c : 0.80\n",
      "bdee441c : 0.77\n",
      "bdee441c : 0.81\n",
      "bdee441c : 0.85\n",
      "bdee441c : 0.86\n",
      "bdee441c : 0.92\n",
      "bdee441c : 0.91\n",
      "bdee441c : 0.93\n",
      "bdee441c : 0.94\n",
      "bdee441c : 0.93\n",
      "bdee441c : 0.89\n",
      "bdee441c : 0.87\n",
      "bdee441c : 0.91\n",
      "bdee441c : 0.82\n",
      "bdee441c : 0.89\n",
      "bdee441c : 0.81\n",
      "bdee441c : 0.76\n",
      "bdee441c : 0.85\n",
      "bdee441c : 0.81\n",
      "bdee441c : 0.81\n",
      "bdee441c : 0.89\n",
      "bdee441c : 0.77\n",
      "bdee441c : 0.88\n",
      "bdee441c : 0.91\n",
      "bdee441c : 0.47\n",
      "bdee441c : 0.88\n",
      "bdee441c : 0.89\n",
      "bdee441c : 0.78\n",
      "bdee441c : 0.72\n",
      "bdee441c : 0.92\n",
      "bdee441c : 0.86\n",
      "bdee441c : 0.86\n",
      "bdee441c : 0.87\n",
      "bdee441c : 0.82\n",
      "bdee441c : 0.77\n",
      "bdee441c : 0.84\n",
      "bdee441c : 0.91\n",
      "bdee441c : 0.82\n",
      "bdee441c : 0.88\n",
      "bdee441c : 0.85\n",
      "bdee441c : 0.90\n",
      "bdee441c : 0.81\n",
      "bdee441c : 0.90\n",
      "bdee441c : 0.81\n",
      "bdee441c : 0.90\n",
      "bdee441c : 0.87\n",
      "bdee441c : 0.75\n",
      "bdee441c : 0.83\n",
      "bdee441c : 0.75\n",
      "bdee441c : 0.78\n",
      "bdee441c : 0.94\n",
      "bdee441c : 0.85\n",
      "bdee441c : 0.88\n",
      "bdee441c : 0.87\n",
      "bdee441c : 0.76\n",
      "bdee441c : 0.85\n",
      "bdee441c : 0.88\n",
      "bdee441c : 0.88\n",
      "bdee441c : 0.85\n",
      "bdee441c : 0.77\n",
      "bdee441c : 0.79\n",
      "bdee441c : 0.74\n",
      "bdee441c : 0.79\n",
      "bdee441c : 0.90\n",
      "bdee441c : 0.76\n",
      "bdee441c : 0.94\n",
      "bdee441c : 0.92\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "print(\"enrolled_spks:{}\".format(enroll_spks))\n",
    "for i, row in dev_uttrs.iterrows():\n",
    "    audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "    test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "    word_label = sent_labels.index(row.sent)\n",
    "#     score = 1 - cosine(test_in, word_spk_models[row.spk][word_label])\n",
    "    score = spk_verify(uni_spk_models, test_in, None)\n",
    "    test_scores.append(score)\n",
    "    print(\"{} : {:.2f}\".format(row.spk, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### different ths for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes: auc: 1.00, eer: 0.00\n",
      "no: auc: 0.99, eer: 0.05\n",
      "up: auc: 0.89, eer: 0.11\n",
      "down: auc: 1.00, eer: 0.04\n",
      "left: auc: 1.00, eer: 0.00\n",
      "right: auc: 0.99, eer: 0.09\n",
      "on: auc: 0.99, eer: 0.05\n",
      "off: auc: 1.00, eer: 0.00\n",
      "stop: auc: 0.94, eer: 0.19\n",
      "go: auc: 1.00, eer: 0.00\n",
      "zero: auc: 0.99, eer: 0.04\n",
      "one: auc: 0.98, eer: 0.05\n",
      "two: auc: 0.97, eer: 0.11\n",
      "three: auc: 1.00, eer: 0.00\n",
      "four: auc: 0.99, eer: 0.04\n",
      "five: auc: 1.00, eer: 0.00\n",
      "six: auc: 0.99, eer: 0.11\n",
      "seven: auc: 1.00, eer: 0.00\n",
      "eight: auc: 0.99, eer: 0.08\n",
      "nine: auc: 0.99, eer: 0.05\n"
     ]
    }
   ],
   "source": [
    "for keyword in main_sents:\n",
    "    word_pos_sample = dev_uttrs[dev_uttrs.sent == keyword]\n",
    "    word_neg_sample = test_uttrs[test_uttrs.sent == keyword]\n",
    "\n",
    "    word_pos_scores = []\n",
    "    word_neg_scores = []\n",
    "    for i, row in word_pos_sample.iterrows():\n",
    "        audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "        test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "        word_label = sent_labels.index(row.sent)\n",
    "        score = spk_verify(word_spk_models, test_in, word_label)\n",
    "        word_pos_scores.append(score)\n",
    "\n",
    "    for i, row in word_neg_sample.iterrows():\n",
    "        audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "        test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "        word_label = sent_labels.index(row.sent)\n",
    "        score = spk_verify(word_spk_models, test_in, word_label)\n",
    "        word_neg_scores.append(score)\n",
    "\n",
    "    dists = np.concatenate((word_pos_scores, word_neg_scores), 0)\n",
    "    labels = np.concatenate((np.ones_like(word_pos_scores), np.zeros_like(word_neg_scores)), 0)\n",
    "    auc_, eer_, _ = roc_auc_eer(dists, labels)\n",
    "    print(\"{}: auc: {:.2f}, eer: {:.2f}\".format(keyword, auc_, eer_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### different ths for each speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def one_spk_verify(spk_model, test_in, sent_label=None, who=False):\n",
    "    best_score = -2\n",
    "    pred_spk = 'Unknown'\n",
    "    is_unknown = True\n",
    "    \n",
    "\n",
    "    if sent_label is not None:\n",
    "        signature = spk_model[sent_label]\n",
    "        signature_uni = np.mean(spk_model[spk],0)\n",
    "        score = max(1-cosine(test_in, signature), 1-cosine(test_in,signature_uni))\n",
    "    else:\n",
    "        signature_uni = spk_model\n",
    "        score = 1-cosine(test_in, signature_uni)\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score     \n",
    "        pred_spk = spk\n",
    "    if who:co\n",
    "        return pred_spk, best_score\n",
    "    else:\n",
    "        return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bdee441c: auc: 1.00, eer: 0.00\n",
      "c120e80e: auc: 0.99, eer: 0.04\n",
      "28ce0c58: auc: 0.96, eer: 0.09\n",
      "d0faf7e4: auc: 0.99, eer: 0.07\n",
      "c1d39ce8: auc: 0.94, eer: 0.12\n"
     ]
    }
   ],
   "source": [
    "for spk in enroll_spks:\n",
    "    spk_pos_sample = dev_uttrs[dev_uttrs.spk==spk]\n",
    "    spk_neg_sample = test_uttrs\n",
    "\n",
    "    spk_pos_scores = []\n",
    "    spk_neg_scores = []\n",
    "    for i, row in spk_pos_sample.iterrows():\n",
    "        audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "        test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "        pred_spk, _ = spk_verify(uni_spk_models, test_in, who=True)\n",
    "        score = one_spk_verify(uni_spk_models[pred_spk], test_in)\n",
    "        spk_pos_scores.append(score)\n",
    "\n",
    "    for i, row in spk_neg_sample.iterrows():\n",
    "        audio_path = os.path.join(data_dir, row.sent, row.file)\n",
    "        test_in = hk.embed(sv_config, sv_model, audio_path)\n",
    "        pred_spk, _ = spk_verify(uni_spk_models, test_in, who=True)\n",
    "        score = one_spk_verify(uni_spk_models[p   red_spk], test_in)\n",
    "        spk_neg_scores.append(score)\n",
    "\n",
    "    dists = np.concatenate((spk_pos_scores, spk_neg_scores), 0)\n",
    "    labels = np.concatenate((np.ones_like(spk_pos_scores), np.zeros_like(spk_neg_scores)), 0)\n",
    "    auc_, eer_, _ = roc_auc_eer(dists, labels)\n",
    "    print(\"{}: auc: {:.2f}, eer: {:.2f}\".format(spk, auc_, eer_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
