{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ideal_kar/Dataset/ASVspoof_2017_Dataset/protocol/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "asv_home_dir = \"/home/ideal_kar/Dataset/ASVspoof_2017_Dataset/\"\n",
    "asv_protocol_dir = os.path.join(asv_home_dir, \"protocol/\")\n",
    "print (asv_protocol_dir)\n",
    "data_type = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ASVspoof_protocol(asvs_home_dir, data_type='dev', separate=False, phrase_filter=None, speaker_filter=None):\n",
    "    '''\n",
    "    \n",
    "    :param separate: Chooses whether to separate genuine speeches\n",
    "    '''\n",
    "    asvs_protocol_dir = os.path.join(asvs_home_dir, \"protocol/\")\n",
    "    file_dir, label, speaker_ID, phrase_ID, env_ID, playback_ID, recording_ID = [], [] ,[] ,[] ,[] ,[] ,[]\n",
    "    if separate==False:\n",
    "        pass\n",
    "    else:\n",
    "        file_dir_s, label_s, speaker_ID_s, phrase_ID_s, env_ID_s, playback_ID_s, recording_ID_s = [], [] ,[] ,[] ,[] ,[] ,[]\n",
    "    \n",
    "    if data_type=='dev':\n",
    "        split_file=os.path.join(asvs_protocol_dir, \"ASVspoof2017_dev.trl.txt\")\n",
    "    elif data_type=='train':\n",
    "        split_file=os.path.join(asvs_protocol_dir, \"ASVspoof2017_train.trn.txt\")\n",
    "    elif data_type=='eval':\n",
    "        split_file=os.path.join(asvs_protocol_dir, \"ASVspoof2017_eval_v2.trl.txt\")\n",
    "        split_key_file = os.path.join(asvs_protocol_dir, \"ASVspoof2017_eval_v2_key.trl.txt\")\n",
    "    else:\n",
    "        split_file=os.path.join(asvs_protocol_dir, \"ASVspoof2017_dev.trl.txt\")\n",
    "    \n",
    "    \n",
    "    if data_type == 'eval':\n",
    "        asvs_file_dir = os.path.join(asvs_home_dir, \"ASVspoof2017_eval/\")\n",
    "        with open(split_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            with open(split_key_file, 'r') as keyf:\n",
    "                key_lines = keyf.readlines()\n",
    "            \n",
    "            for key_line, line in zip(key_lines, lines):\n",
    "                parsed_key_line = key_line.rstrip().split(' ')\n",
    "                parsed_line = line.rstrip().split(' ')\n",
    "                \n",
    "                if separate==False:\n",
    "                    if parsed_key_line[1] == 'genuine':\n",
    "                        label.append(1)\n",
    "                    elif parsed_key_line[1] == 'spoof':\n",
    "                        label.append(0)\n",
    "                    file_dir.append(os.path.join(asvs_file_dir, parsed_line[0]))\n",
    "                \n",
    "                    phrase_ID.append(int(parsed_line[1][1:]))\n",
    "                else:\n",
    "                    if parsed_key_line[1] == 'genuine':\n",
    "                        label.append(1)\n",
    "                        file_dir.append(os.path.join(asvs_file_dir, parsed_line[0]))\n",
    "                \n",
    "                        phrase_ID.append(int(parsed_line[1][1:]))\n",
    "                    elif parsed_key_line[1] == 'spoof':\n",
    "                        label_s.append(0)\n",
    "                        file_dir_s.append(os.path.join(asvs_file_dir, parsed_line[0]))\n",
    "                \n",
    "                        phrase_ID_s.append(int(parsed_line[1][1:]))\n",
    "        \n",
    "        if separate==False:\n",
    "            return filter_ASVspoof([file_dir, label, phrase_ID], phrase_filter, speaker_filter)\n",
    "        else:\n",
    "            return [filter_ASVspoof([file_dir, label, phrase_ID], phrase_filter, speaker_filter), \\\n",
    "                    filter_ASVspoof([file_dir_s, label_s, phrase_ID_s], phrase_filter, speaker_filter)]\n",
    "    else:\n",
    "        if data_type == 'dev':\n",
    "            asvs_file_dir = os.path.join(asv_home_dir, \"ASVspoof2017_dev/\")\n",
    "        elif data_type == 'train':\n",
    "            asvs_file_dir = os.path.join(asv_home_dir, \"ASVspoof2017_train/\")\n",
    "        else:\n",
    "            asvs_file_dir = os.path.join(asv_home_dir, \"ASVspoof2017_dev/\")\n",
    "        \n",
    "        with open(split_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parsed_line = line.rstrip().split(' ')\n",
    "                \n",
    "                if separate == False:\n",
    "                    file_dir.append(os.path.join(asvs_file_dir, parsed_line[0]))\n",
    "                    \n",
    "                    speaker_ID.append(int(parsed_line[2][1:]))\n",
    "                    \n",
    "                    phrase_ID.append(int(parsed_line[3][1:]))\n",
    "                    \n",
    "                    if parsed_line[1] == 'genuine':\n",
    "                        label.append(1)\n",
    "                        env_ID.append(-1)\n",
    "                        playback_ID.append(-1)\n",
    "                        recording_ID.append(-1)\n",
    "                    elif parsed_line[1] == 'spoof':\n",
    "                        label.append(0)\n",
    "                        env_ID.append(int(parsed_line[4][1:]))\n",
    "                        playback_ID.append(int(parsed_line[5][1:]))\n",
    "                        recording_ID.append(int(parsed_line[6][1:]))\n",
    "                else:\n",
    "                    if parsed_line[1] == 'genuine':\n",
    "                        label.append(1)\n",
    "                        env_ID.append(-1)\n",
    "                        playback_ID.append(-1)\n",
    "                        recording_ID.append(-1)\n",
    "                        file_dir.append(os.path.join(asvs_file_dir, parsed_line[0]))\n",
    "                        speaker_ID.append(int(parsed_line[2][1:])) \n",
    "                        phrase_ID.append(int(parsed_line[3][1:]))\n",
    "                    elif parsed_line[1] == 'spoof':\n",
    "                        label_s.append(0)\n",
    "                        env_ID_s.append(int(parsed_line[4][1:]))\n",
    "                        playback_ID_s.append(int(parsed_line[5][1:]))\n",
    "                        recording_ID_s.append(int(parsed_line[6][1:]))\n",
    "                        file_dir_s.append(os.path.join(asvs_file_dir, parsed_line[0]))\n",
    "                        speaker_ID_s.append(int(parsed_line[2][1:])) \n",
    "                        phrase_ID_s.append(int(parsed_line[3][1:]))\n",
    "        if separate == False:            \n",
    "            return filter_ASVspoof([file_dir, label, phrase_ID, speaker_ID, env_ID, playback_ID, recording_ID], phrase_filter, speaker_filter)\n",
    "        else:\n",
    "            return [filter_ASVspoof([file_dir, label, phrase_ID, speaker_ID, env_ID, playback_ID, recording_ID], phrase_filter, speaker_filter), \\\n",
    "                    filter_ASVspoof([file_dir_s, label_s, phrase_ID_s, speaker_ID_s, env_ID_s, playback_ID_s, recording_ID_s], phrase_filter, speaker_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508\n"
     ]
    }
   ],
   "source": [
    "temp = read_ASVspoof_protocol(asv_home_dir, separate=True, data_type='train')\n",
    "print(len(temp[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_ASVspoof_by_phrase_ID(target_lists, filter_IDs):\n",
    "    \n",
    "    '''\n",
    "    Exception handling for non-list typed inputs\n",
    "    '''\n",
    "    if type(filter_IDs).__name__=='int':\n",
    "        filter_IDs = [filter_IDs]\n",
    "    \n",
    "    elif type(filter_IDs).__name__=='tuple':\n",
    "        filter_IDs = list(filter_IDs)\n",
    "    \n",
    "    phrase_IDs = target_lists[2]\n",
    "    \n",
    "    matched_indices = []\n",
    "    \n",
    "    for index, phrase_ID in enumerate(phrase_IDs):\n",
    "        if phrase_ID in filter_IDs:\n",
    "            matched_indices.append(index)\n",
    "    \n",
    "    if len(matched_indices) == 0:\n",
    "        raise BaseException(\"No matching results\")\n",
    "    \n",
    "    target_length = list(range(len(target_lists)))\n",
    "    \n",
    "    for i in target_length:\n",
    "        target_lists[i] = [target_lists[i][j] for j in matched_indices]\n",
    "        \n",
    "    return target_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_ASVspoof_by_speaker_ID(target_lists, filter_IDs):\n",
    "    \n",
    "    '''\n",
    "    Exception handling for non-list typed inputs\n",
    "    '''\n",
    "    if type(filter_IDs).__name__=='int':\n",
    "        filter_IDs = [filter_IDs]\n",
    "    \n",
    "    elif type(filter_IDs).__name__=='tuple':\n",
    "        filter_IDs = list(filter_IDs)\n",
    "        \n",
    "    if len(target_lists) < 4:\n",
    "        raise BaseException(\"Cannot apply spk filtering to eval dataset\")\n",
    "    \n",
    "    speaker_IDs = target_lists[3]\n",
    "    \n",
    "    matched_indices = []\n",
    "    \n",
    "    for index, speaker_ID in enumerate(speaker_IDs):\n",
    "        if speaker_ID in filter_IDs:\n",
    "            matched_indices.append(index)\n",
    "    \n",
    "    if len(matched_indices) == 0:\n",
    "        raise BaseException(\"No matching results\")\n",
    "    \n",
    "    target_length = list(range(len(target_lists)))\n",
    "    \n",
    "    for i in target_length:\n",
    "        target_lists[i] = [target_lists[i][j] for j in matched_indices]\n",
    "        \n",
    "    return target_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_ASVspoof(target_lists, phrase_filter_IDs, speaker_filter_IDs):\n",
    "    if phrase_filter_IDs!=None:\n",
    "        target_lists = filter_ASVspoof_by_phrase_ID(target_lists, phrase_filter_IDs)\n",
    "    if speaker_filter_IDs != None:\n",
    "        target_lists = filter_ASVspoof_by_speaker_ID(target_lists, speaker_filter_IDs)\n",
    "    return target_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ASVspoof_protocol_wo_eval(merge=True, asvs_home_dir='/home/ideal_kar/Dataset/ASVspoof_2017_Dataset/', phrase_filter=None, speaker_filter=None):\n",
    "    \n",
    "    \n",
    "    dev_lists = read_ASVspoof_protocol(asvs_home_dir, data_type='dev', phrase_filter=phrase_filter, speaker_filter=speaker_filter)\n",
    "    train_lists = read_ASVspoof_protocol(asvs_home_dir, data_type='train', phrase_filter=phrase_filter, speaker_filter=speaker_filter)\n",
    "    \n",
    "    if merge:\n",
    "        return [dev_lists[i] + train_lists[i] for i in range(len(dev_lists))]\n",
    "    else:\n",
    "        return [dev_lists, train_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ASVspoof_for_iden(asvs_home_dir='/home/ideal_kar/Dataset/ASVspoof_2017_Dataset/', separate=True, phrase_filter=None, speaker_filter=None):\n",
    "    \n",
    "    dev_lists = read_ASVspoof_protocol(asvs_home_dir, data_type='dev', separate=separate, phrase_filter=phrase_filter, speaker_filter=speaker_filter)\n",
    "    train_lists = read_ASVspoof_protocol(asvs_home_dir, data_type='train', separate=separate, phrase_filter=phrase_filter, speaker_filter=speaker_filter)\n",
    "    \n",
    "    if separate==False:\n",
    "        return [dev_lists[i] + train_lists[i] for i in range(len(dev_lists))]\n",
    "    else:\n",
    "        return [[dev_lists[j][i] + train_lists[j][i] for i in range(len(dev_lists[0]))] for j in range(len(dev_lists))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ASVspoof_for_veri(asvs_home_dir='/home/ideal_kar/Dataset/ASVspoof_2017_Dataset/', phrase_filter=None, speaker_filter=None):\n",
    "    separate = False\n",
    "    \n",
    "    dev_lists = read_ASVspoof_protocol(asvs_home_dir, data_type='dev', separate=separate, phrase_filter=phrase_filter, speaker_filter=speaker_filter)\n",
    "    train_lists = read_ASVspoof_protocol(asvs_home_dir, data_type='train', separate=separate, phrase_filter=phrase_filter, speaker_filter=speaker_filter)\n",
    "    \n",
    "    return [dev_lists[i] + train_lists[i] for i in range(len(dev_lists))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "parser = argparse.ArgumentParser(description='make manifest for designated dataset')\n",
    "parser.add_argument('--dataset', default='voxceleb', help='voxceleb, ASVspoof available')\n",
    "parser.add_argument('--voxc_dir', metavar='DIR',\n",
    "                    help='your path to voxceleb dataset', default=\"/home/ideal_kar/Dataset/voxceleb/\")\n",
    "parser.add_argument('--voxc_split_file', metavar='DIR',\n",
    "                    help='path to voxceleb split csv', default=\"/home/ideal_kar/Dataset/voxceleb/Identification_split.txt\")\n",
    "parser.add_argument('--nb_class', default=30, type=int, help='number of classes for voxceleb')\n",
    "parser.add_argument('--ASVs_dir', metavar='DIR', help='your path to ASVspoof dataset', default=\"/home/ideal_kar/Dataset/ASVspoof/\")\n",
    "parser.add_argument('--speaker_filt', nargs='+', default=None, help='select speaker IDs for ASVspoof')\n",
    "parser.add_argument('--phrase_filt', nargs='+', default=None, help='select phrase IDs for ASVspoof')\n",
    "parser.add_argument('--work_type', default='iden', help='iden/veri for voxceleb, d(dev)/t(train)/dt(d&t)/eval/iden/aspoof for ASVspoof')\n",
    "parser.add_argument('--separate', default='true', help='true: train only with genuine speech / false: all shuffled')\n",
    "parser.add_argument('--TestPOI', default=None, nargs='+', help='voxceleb: testPOI 1st characters / ASVspoof: POI speaker IDs')\n",
    "\n",
    "def read_voxc_speakers(split_file):\n",
    "    speakers = {}\n",
    "    with open(split_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        id = 0\n",
    "        for line in lines:\n",
    "            set_id, file_path = line.split()\n",
    "            speaker, file_name = file_path.split('/')\n",
    "            if speaker not in speakers:\n",
    "                speakers[speaker] = id\n",
    "                id += 1\n",
    "    return speakers\n",
    "\n",
    "def read_voxc_iden_split(split_file, testPOI):\n",
    "    speakers = read_speakers(split_file)\n",
    "    trainX, valX, testX = [], [], []\n",
    "    trainY, valY, testY = [], [], []\n",
    "\n",
    "    with open(split_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            set_id, file_path = line.split()\n",
    "            set_id = int(set_id)\n",
    "            speaker, file_name = file_path.split('/')\n",
    "            # avoid testPOI\n",
    "            if speaker[0] == testPOI: continue\n",
    "            if set_id == 1:\n",
    "                trainX.append(file_path)\n",
    "                trainY.append(speakers[speaker])\n",
    "            elif set_id == 2:\n",
    "                valX.append(file_path)\n",
    "                valY.append(speakers[speaker])\n",
    "            else:\n",
    "                testX.append(file_path)\n",
    "                testY.append(speakers[speaker])\n",
    "\n",
    "    trainX, trainY = shuffle(trainX, trainY)\n",
    "    valX, valY = shuffle(valX, valY)\n",
    "    testX, testY = shuffle(testX, testY)\n",
    "\n",
    "    return (np.array(trainX), np.array(trainY)), \\\n",
    "           (np.array(valX), np.array(valY)), (np.array(testX), np.array(testY)), \\\n",
    "           speakers\n",
    "\n",
    "def read_voxc_veri_split(split_file, nb_speakers):\n",
    "    speaker_id = read_voxc_speakers(split_file)\n",
    "    speaker_names = set(speaker_id.keys())\n",
    "\n",
    "    test_speakers = set([spk for spk in speaker_names if spk[0] == 'E'])\n",
    "    train_speakers = speaker_names - test_speakers\n",
    "\n",
    "    test_speakers = [speaker_id[spk] for spk in test_speakers]\n",
    "    train_speakers = [speaker_id[spk] for spk in train_speakers]\n",
    "\n",
    "    test_speakers = np.random.choice(test_speakers, size=int(nb_speakers/5))\n",
    "    train_speakers  = np.random.choice(train_speakers, size=nb_speakers)\n",
    "\n",
    "    trainX, testX = [], []\n",
    "    trainY, testY = [], []\n",
    "\n",
    "    with open(split_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            set_id, file_path = line.split()\n",
    "            set_id = int(set_id)\n",
    "            speaker, file_name = file_path.split('/')\n",
    "            if speaker[0] != 'E':\n",
    "                trainX.append(file_path)\n",
    "                trainY.append(speaker_id[speaker])\n",
    "            else:\n",
    "                testX.append(file_path)\n",
    "                testY.append(speaker_id[speaker])\n",
    "\n",
    "    trainX, trainY = shuffle(trainX, trainY)\n",
    "    testX, testY = shuffle(testX, testY)\n",
    "\n",
    "    return (np.array(trainX), np.array(trainY)), \\\n",
    "           (np.array(testX), np.array(testY)), \\\n",
    "           (speaker_id, test_speakers, train_speakers)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    dataset = args.dataset\n",
    "    work_type = args.work_type\n",
    "    nb_class = args.nb_class\n",
    "    testPOI = args.testPOI\n",
    "    \n",
    "    if dataset == 'voxceleb':\n",
    "        data_dir = args.voxc_dir\n",
    "        if work_type == 'iden':\n",
    "            (trainX, trainY), (valX, valY), (testX, testY), speakers = read_voxc_iden_split(args.voxc_split_file, testPOI)\n",
    "            nb_class = args.nb_class\n",
    "            work_type = args.work_type\n",
    "            datasets = {'train':trainX, 'val':valX, 'test':testX}\n",
    "            Ys = {'train':trainY, 'val':valY, 'test':testY}\n",
    "            for dataset in datasets.keys():\n",
    "                with open(\"data/voxc_{}_{}_manifest.csv\".format(dataset, work_type), \"w\") as f:\n",
    "                    for wav_path in datasets[dataset]:\n",
    "                        speaker = wav_path.split('/')[0]\n",
    "                        if speakers[speaker] > nb_class-1: continue\n",
    "                        line = ','.join([args.voxc_dir+wav_path, str(speakers[speaker])])\n",
    "                        f.write(line)\n",
    "                        f.write('\\n')\n",
    "        elif work_type == 'veri':\n",
    "            # TODO: negative sampling, hard negative sampling\n",
    "            (trainX, trainY), (testX, testY), (speaker_id, test_speakers, train_speakers)\\\n",
    "                    = read_voxc_veri_split(args.voxc_split_file, args.nb_class, testPOI)\n",
    "            Xs = {'train':trainX,'test':testX}\n",
    "            Ys = {'train':trainY, 'test':testY}\n",
    "            speakers = {'train':train_speakers, 'test':test_speakers}\n",
    "            for dataset in Xs.keys():\n",
    "                X = Xs[dataset]\n",
    "                Y = Ys[dataset]\n",
    "                data_indices = [np.where(Y == id)[0] for id in speakers[dataset]]\n",
    "                n = min([len(idx) for idx in data_indices]) - 1\n",
    "                x0_data = []\n",
    "                x1_data = []\n",
    "                labels = []\n",
    "                total_speakers = len(data_indices)\n",
    "                for d in range(total_speakers):\n",
    "                    for i in range(n):\n",
    "                        # same label\n",
    "                        x0_data.append(X[data_indices[d][i]])\n",
    "                        x1_data.append(X[data_indices[d][i+1]])\n",
    "                        labels.append('1')\n",
    "                        # different label\n",
    "                        while(1):\n",
    "                            dn = np.random.choice(total_speakers,1)[0]\n",
    "                            if dn != d: break\n",
    "                        x0_data.append(X[data_indices[d][i]])\n",
    "                        x1_data.append(X[data_indices[dn][i]])\n",
    "                        labels.append('0')\n",
    "                lines = []\n",
    "                base_dir = '/home/muncok/DL/dataset/voxceleb/'\n",
    "                for x0, x1, label in zip(x0_data, x1_data, labels):\n",
    "                    x0 = base_dir + x0\n",
    "                    x1 = base_dir + x1\n",
    "                    line = ','.join([x0, x1, label])\n",
    "                    lines.append(line)\n",
    "                with open(\"data/voxc_{}_{}_manifest.csv\".format(dataset, work_type), \"w\") as f:\n",
    "                    wr = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "                    wr.writerow(lines)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    else:\n",
    "        data_dir = args.ASVs_dir\n",
    "        speaker_filt = args.speaker_filt\n",
    "        phrase_filt = args.phrase_filt\n",
    "        if work_type=='d':\n",
    "            data_lists = read_ASVspoof_protocol(data_dir, 'dev', phrase_filter=phrase_filt, speaker_filter=speaker_filt)\n",
    "            \n",
    "            shuffle_index = shuffle(list(range(len(data_lists[0]))))\n",
    "            data_lists = [[data_lists[i][j] for j in shuffle_index] for i in range(len(data_lists))]\n",
    "            with open(\"data/ASVs_raw_dev_manifest.csv\", \"w\") as f:\n",
    "                for i in range(len(data_lists[0])):\n",
    "                    line=''\n",
    "                    for dev_list in data_lists:\n",
    "                        line=line + str(dev_list[i]) + ','\n",
    "                    f.write(line[:-1])\n",
    "                    f.write('\\n')\n",
    "                    \n",
    "        elif work_type=='t':\n",
    "            data_lists = read_ASVspoof_protocol(data_dir, data_type='train', phrase_filter=phrase_filt, speaker_filter=speaker_filt)\n",
    "            \n",
    "            shuffle_index = shuffle(list(range(len(data_lists[0]))))\n",
    "            data_lists = [[data_lists[i][j] for j in shuffle_index] for i in range(len(data_lists))]\n",
    "            with open(\"data/ASVs_raw_train_manifest.csv\", \"w\") as f:\n",
    "                for i in range(len(data_lists[0])):\n",
    "                    line=''\n",
    "                    for train_list in data_lists:\n",
    "                        line=line + str(train_list[i]) + ','\n",
    "                    f.write(line[:-1])\n",
    "                    f.write('\\n')\n",
    "                    \n",
    "        elif work_type=='dt':\n",
    "            data_lists = read_ASVspoof_protocol_wo_eval(asvs_home_dir=data_dir, phrase_filter=phrase_filt, speaker_filter=speaker_filt)\n",
    "            \n",
    "            shuffle_index = shuffle(list(range(len(data_lists[0]))))\n",
    "            data_lists = [[data_lists[i][j] for j in shuffle_index] for i in range(len(data_lists))]\n",
    "            with open(\"data/ASVs_raw_devtrain_manifest.csv\", \"w\") as f:\n",
    "                for i in range(len(data_lists[0])):\n",
    "                    line=''\n",
    "                    for dt_list in data_lists:\n",
    "                        line=line + str(dt_list[i]) + ','\n",
    "                    f.write(line[:-1])\n",
    "                    f.write('\\n')\n",
    "                    \n",
    "        elif work_type=='eval':\n",
    "            data_lists = read_ASVspoof_protocol(data_dir, 'eval', phrase_filter=phrase_filt)\n",
    "            \n",
    "            shuffle_index = shuffle(list(range(len(data_lists[0]))))\n",
    "            data_lists = [[data_lists[i][j] for j in shuffle_index] for i in range(len(data_lists))]\n",
    "            with open(\"data/ASVs_raw_eval_manifest.csv\", \"w\") as f:\n",
    "                for i in range(len(data_lists[0])):\n",
    "                    line=''\n",
    "                    for eval_list in data_lists:\n",
    "                        line=line + str(eval_list[i]) + ','\n",
    "                    f.write(line[:-1])\n",
    "                    f.write('\\n')\n",
    "                    \n",
    "        elif work_type=='iden':\n",
    "            separate = True if args.separate=='true' else False\n",
    "            data_lists = read_ASVspoof_for_iden(separate=separate, phrase_filter=phrase_filt, speaker_filter=speaker_filt)\n",
    "  \n",
    "            if testPOI is not None:\n",
    "                testPOI = [int(testPOI[i]) for i in range(len(testPOI))]\n",
    "                if speaker_filt is None: \n",
    "                    speaker_filt = []\n",
    "                \n",
    "                if any(POI in speaker_filt for POI in testPOI):\n",
    "                    raise BaseException(\"Some of testPOIs are already excluded by speaker filtering\")\n",
    "            else:\n",
    "                testPOI=[]\n",
    "            \n",
    "            if separate==False:\n",
    "                total_length = len(data_lists[0])\n",
    "                shuffle_index = shuffle(list(range(len(data_lists[0]))))\n",
    "                data_lists = [[data_lists[i][j] for j in shuffle_index if data_lists[3][j] not in testPOI] for i in range(len(data_lists))]\n",
    "                \n",
    "                train_lists = [[data_lists[i][:total_length/2]] for i in range(len(data_lists))]\n",
    "                test_lists = [[data_lists[i][total_length/2:]] for i in range(len(data_lists))]\n",
    "                \n",
    "            else:\n",
    "                train_lists = data_lists[0]\n",
    "                test_lists = data_lists[1]\n",
    "                \n",
    "                shuffle_index = shuffle(list(range(len(train_lists[0]))))\n",
    "                train_lists = [[train_lists[i][j] for j in shuffle_index if train_lists[3][j] not in testPOI] for i in range(len(train_lists))]\n",
    "                \n",
    "                shuffle_index = shuffle(list(range(len(test_lists[0]))))\n",
    "                test_lists = [[test_lists[i][j] for j in shuffle_index if test_lists[3][j] not in testPOI] for i in range(len(test_lists))]\n",
    "                \n",
    "            with open(\"data/ASVs_iden_train_manifest.csv\", \"w\") as f:\n",
    "                for i in range(len(train_lists[0])):\n",
    "                    line=''\n",
    "                    for train_list in train_lists:\n",
    "                        line=line + str(train_list[i]) + ','\n",
    "                    f.write(line[:-1])\n",
    "                    f.write('\\n')\n",
    "            \n",
    "            with open(\"data/ASVs_iden_test_manifest.csv\", \"w\") as f:\n",
    "                for i in range(len(test_lists[0])):\n",
    "                    line=''\n",
    "                    for test_list in test_lists:\n",
    "                        line=line + str(test_list[i]) + ','\n",
    "                    f.write(line[:-1])\n",
    "                    f.write('\\n')\n",
    "        \n",
    "        elif work_type=='veri':\n",
    "            data_lists = read_ASVspoof_for_veri(phrase_filter=phrase_filt, speaker_filter=speaker_filt)\n",
    "            \n",
    "            \n",
    "        elif work_type=='aspoof':\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            raise BaseException('Invalid work type')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1]\n",
      "[[3, 1, 2], [3, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 2, 3], [1, 2, 3]]\n",
    "\n",
    "shuffle_index = list(range(len(a[0])))\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "shuffle(shuffle_index)\n",
    "\n",
    "print(shuffle_index)\n",
    "\n",
    "a = [[a[i][j] for j in shuffle_index ] for i in range(len(a))]\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_ASVspoof_protocol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2ab6816a63ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_ASVspoof_protocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_ASVspoof_protocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(len(filter_by_phrase_ID(temp, [11])[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_by_speaker_ID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(len(filter_by_phrase_ID(read_ASVspoof_protocol(data_type='eval'), [1, 2])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_ASVspoof_protocol' is not defined"
     ]
    }
   ],
   "source": [
    "temp = read_ASVspoof_protocol(data_type='dev')\n",
    "print(len(read_ASVspoof_protocol(data_type='dev')[0]))\n",
    "#print(len(filter_by_phrase_ID(temp, [11])[0]))\n",
    "print(len(filter_by_speaker_ID(temp, [18])[0]))\n",
    "#print(len(filter_by_phrase_ID(read_ASVspoof_protocol(data_type='eval'), [1, 2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "a = ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 1, 3, 1, 4, 1, 5, 1, 6])\n",
    "filters = [1]\n",
    "matched_indices = []\n",
    "for index, value in enumerate(a[1]):\n",
    "    if value in filters:\n",
    "        matched_indices.append(index)\n",
    "\n",
    "filtered_list = []\n",
    "filtered_list = [a[1][i] for i in matched_indices]\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 1, 2, 1, 2]\n",
    "b = []\n",
    "print(any(item in b for item in a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]])\n",
      "([[1300, 1400, 1500], [1600, 1700, 1800]], [[700, 800, 900], [1000, 1100, 1200]])\n",
      "([[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]])\n",
      "([[1300, 1400, 1500], [1600, 1700, 1800]], [[100, 200, 300], [400, 500, 600]])\n",
      "([[7, 8, 9], [10, 11, 12]], [[1, 2, 3], [4, 5, 6]])\n",
      "([[700, 800, 900], [1000, 1100, 1200]], [[1300, 1400, 1500], [1600, 1700, 1800]])\n",
      "([[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]])\n",
      "([[700, 800, 900], [1000, 1100, 1200]], [[100, 200, 300], [400, 500, 600]])\n",
      "([[13, 14, 15], [16, 17, 18]], [[1, 2, 3], [4, 5, 6]])\n",
      "([[100, 200, 300], [400, 500, 600]], [[1300, 1400, 1500], [1600, 1700, 1800]])\n",
      "([[13, 14, 15], [16, 17, 18]], [[7, 8, 9], [10, 11, 12]])\n",
      "([[100, 200, 300], [400, 500, 600]], [[700, 800, 900], [1000, 1100, 1200]])\n"
     ]
    }
   ],
   "source": [
    "a = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]]]\n",
    "b = [[[1300, 1400, 1500], [1600, 1700, 1800]], [[700, 800, 900], [1000, 1100, 1200]], [[100, 200, 300], [400, 500, 600]]]\n",
    "import itertools\n",
    "p_a = itertools.permutations(a, 2)\n",
    "p_b = itertools.permutations(b, 2)\n",
    "for pa, pb in zip(p_a, p_b):\n",
    "    print(pa)\n",
    "    print(pb)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
