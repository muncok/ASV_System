{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muncok/anaconda3/envs/pytorch/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f', 'sample', 'random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import pandas\n",
    "import os\n",
    "os.sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Command Dataset\n",
    "Twenty core command words were recorded, with most speakers saying each\n",
    "of them five times. \n",
    "\n",
    "The core words are \"Yes\", \"No\", \"Up\", \"Down\", \"Left\",\n",
    "\"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\",\n",
    "\"Five\", \"Six\", \"Seven\", \"Eight\", and \"Nine\". To help distinguish unrecognized\n",
    "words, \n",
    "\n",
    "there are also ten auxiliary words, which most speakers only said once.\n",
    "These include \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\",\n",
    "\"Tree\", and \"Wow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "root = \"/home/muncok/DL/dataset/speech_commands/\"\n",
    "wav_files = {}\n",
    "all_spks = []\n",
    "for dirpath, _, filenames in os.walk(root):\n",
    "    if dirpath == root or '_background_noise_' in dirpath: continue\n",
    "    for name in filenames:\n",
    "        sent = dirpath.split('/')[-1]\n",
    "        spk,_,seqID = name.rstrip('.wav').split('_')\n",
    "        uniqID = sent+spk+seqID\n",
    "        all_spks.append(spk)\n",
    "        wav_files[uniqID] = (spk, sent, name)\n",
    "        cnt += 1\n",
    "all_spks = list(set(all_spks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame.from_dict(wav_files, 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set df's column names\n",
    "df.columns = ['spk', 'sent', 'file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read set devision lists\n",
    "test_list = []\n",
    "with open(\"/home/muncok/DL/dataset/speech_commands/testing_list.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        sent, file = line.rstrip('.wav\\n').split('/')\n",
    "        spk,_,seqID = file.split('_')\n",
    "        uniqID = sent+spk+seqID\n",
    "        test_list.append(uniqID)\n",
    "\n",
    "val_list = []\n",
    "with open(\"/home/muncok/DL/dataset/speech_commands/validation_list.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        sent, file = line.rstrip('.wav\\n').split('/')\n",
    "        spk,_,seqID = file.split('_')\n",
    "        uniqID = sent+spk+seqID\n",
    "        val_list.append(uniqID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_columns = []\n",
    "for idx in df.index:\n",
    "    if idx in test_list:\n",
    "        set_columns.append('test')\n",
    "    elif idx in val_list:\n",
    "        set_columns.append('val')\n",
    "    else:\n",
    "        set_columns.append('train')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add set column\n",
    "df = df.assign(set = set_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if train, val, and test are disjoint along spk with same sent\n",
    "all_sents = [\n",
    "             \"Yes\", \"No\", \"Up\", \"Down\", \"Left\",\n",
    "             \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\",\n",
    "             \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\",\n",
    "             \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\",\n",
    "             \"Tree\",\"Wow\"\n",
    "            ]\n",
    "all_sents = list(map(lambda x: x.lower(), all_sents))\n",
    "\n",
    "for sent in all_sents:\n",
    "    train_sent = df[(df.sent == sent) & (df.set=='train')].spk\n",
    "    val_sent = df[(df.sent == sent) & (df.set=='val')].spk\n",
    "    test_sent = df[(df.sent == sent) & (df.set=='test')].spk\n",
    "    assert not set(train_sent) & set(val_sent) &set(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of speakers: 1881\n",
      "kinds of sents: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"number of speakers: {}\".format(len(all_spks)))\n",
    "print(\"kinds of sents: {}\".format(len(all_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sent = 'on'\n",
    "excluded_spks = df[(df.sent == target_sent)].spk.value_counts().index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# backg_spks = random.choices(list(set(all_spks) - set(target_spks)), k=100)\n",
    "backg_spks = list(set(all_spks) - set(excluded_spks))\n",
    "backg_speechs = df[(df.spk.isin(backg_spks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "backg_train = backg_speechs[backg_speechs.set == 'train']\n",
    "backg_val = backg_speechs[backg_speechs.set == 'val']\n",
    "backg_test = backg_speechs[backg_speechs.set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "valid_sent = all_sents.index(target_sent) # only core keywords valid \n",
    "unkown_prob = 0.1\n",
    "unkown_files = []\n",
    "tags = ['train', 'val', 'test']\n",
    "sets = {'train':backg_train, 'val':backg_val, 'test':backg_test}\n",
    "for tag in tags:\n",
    "    samples = []\n",
    "    with open('kws_command_{}_manifest.csv'.format(tag), 'w') as f:\n",
    "        for index, row in sets[tag].iterrows():\n",
    "            file_path = os.path.join(root, row.sent, row.file)\n",
    "            label = all_sents.index(row.sent) # 0,1 for speacial purpose\n",
    "            sample = ','.join([file_path, str(0)])\n",
    "            if label != valid_sent:\n",
    "                sample = ','.join([file_path, str(1)])\n",
    "                unkown_files.append(sample)\n",
    "            else:\n",
    "                sample = ','.join([file_path, str(2)])\n",
    "                samples.append(sample)\n",
    "        nb_unkowns = int(len(samples) * unkown_prob)\n",
    "        random.shuffle(unkown_files)\n",
    "        for _ in range(nb_unkowns):\n",
    "            samples.append(unkown_files.pop())\n",
    "        random.shuffle(samples)\n",
    "        writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "        writer.writerow(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech per a speaker\n",
    "speech_counts = backg_speechs['spk'].groupby(backg_speechs.spk).agg(['count']).sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sv_speechs = backg_speechs.sample(frac=1.0)\n",
    "length = len(sv_speechs)\n",
    "bound = int(0.1 *length)\n",
    "sv_test = sv_speechs[:bound]\n",
    "sv_val = sv_speechs[bound:2*bound]\n",
    "sv_train = sv_speechs[2*bound:]\n",
    "speakers = pandas.concat([sv_test.spk ,sv_train.spk, sv_val.spk]).unique()\n",
    "valid_spk = 1000\n",
    "unkown_prob = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restricting the sentences\n",
    "sent_counts = df.sent.value_counts()\n",
    "chosen_sent = sent_counts.index[:20].tolist()\n",
    "sv_test = df[(df.sent.isin(chosen_sent)) & (df.set == 'test')]\n",
    "sv_val = df[(df.sent.isin(chosen_sent)) & (df.set == 'val')]\n",
    "sv_train = df[(df.sent.isin(chosen_sent)) & (df.set == 'train')]\n",
    "speakers = pandas.concat([sv_test.spk ,sv_train.spk, sv_val.spk]).unique().tolist()\n",
    "valid_spk = np.inf\n",
    "unkown_prob = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "unkown_files = []\n",
    "tags = ['train', 'val', 'test']\n",
    "sets = {'train':sv_train, 'val':sv_val, 'test':sv_test}\n",
    "for tag in tags:\n",
    "    samples = []\n",
    "    with open('sv_command_{}_manifest.csv'.format(tag), 'w') as f:\n",
    "        for index, row in sets[tag].iterrows():\n",
    "            file_path = os.path.join(root, row.sent, row.file)\n",
    "            label = speakers.index(row.spk) \n",
    "            if label > valid_spk - 1:\n",
    "                sample = ','.join([file_path, str(1)])\n",
    "                unkown_files.append(sample)\n",
    "            else:\n",
    "                sample = ','.join([file_path, str(label+2)]) # 0,1 for speacial purpose\n",
    "                samples.append(sample)\n",
    "        nb_unkowns = int(len(samples) * unkown_prob)\n",
    "        random.shuffle(unkown_files)\n",
    "        for _ in range(nb_unkowns):\n",
    "            samples.append(unkown_files.pop())\n",
    "        random.shuffle(samples)\n",
    "        writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "        writer.writerow(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWS+SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sent = \"on\"\n",
    "target_sent_label = all_sents.index(target_sent)\n",
    "# target_spks = random.choices(spks_descending[:valid_spk], k=5) # relaxed condition\n",
    "target_spks = excluded_spks.tolist()\n",
    "target_spks_speech = df[df.spk.isin(target_spks)] # not used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_samples = []\n",
    "for spk in target_spks:\n",
    "    enroll_samples.append(target_spks_speech.sample(5))\n",
    "\n",
    "enroll_samples = pandas.concat(enroll_samples)\n",
    "\n",
    "import csv\n",
    "samples = []\n",
    "with open('system_enroll_manifest.csv'.format(tag), 'w') as f:\n",
    "    for index, row in enroll_samples.iterrows():\n",
    "        file_path = os.path.join(root, row.sent, row.file)\n",
    "        label = target_spks.index(row.spk)\n",
    "        sample = ','.join([file_path, str(label)])\n",
    "        samples.append(sample)\n",
    "    random.shuffle(samples)\n",
    "    writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "    writer.writerow(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_samples = target_spks_speech[(target_spks_speech.sent == target_sent) & \n",
    "                                 (~target_spks_speech.index.isin(enroll_samples.index))]\n",
    "nb_pos_samples = len(pos_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "IC_samples = df[(df.set == 'test') & (~df.spk.isin(target_spks)) & (df.sent == target_sent)].sample(nb_pos_samples)\n",
    "IW_samples = df[(df.set == 'test') & (df.spk.isin(target_spks)) & (df.sent != target_sent) ].sample(nb_pos_samples)\n",
    "TW_samples = df[(df.set == 'test') & (~df.spk.isin(target_spks)) & (df.sent != target_sent)].sample(nb_pos_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pos_samples.index) & set(IC_samples.index) & set(IW_samples.index) & set(TW_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = pandas.concat([IC_samples, IW_samples, TW_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "samples = []\n",
    "with open('system_test_manifest.csv'.format(tag), 'w') as f:\n",
    "    for index, row in pos_samples.iterrows():\n",
    "        file_path = os.path.join(root, row.sent, row.file)\n",
    "        label = 1 # positive sample\n",
    "        sample = ','.join([file_path, str(label)])\n",
    "        samples.append(sample)\n",
    "    for index, row in neg_samples.iterrows():\n",
    "        file_path = os.path.join(root, row.sent, row.file)\n",
    "        label = 0 # negative sample, unknown\n",
    "        sample = ','.join([file_path, str(label)])\n",
    "        samples.append(sample)\n",
    "    random.shuffle(samples)\n",
    "    writer = csv.writer(f, delimiter='\\n', quoting=csv.QUOTE_NONE)\n",
    "    writer.writerow(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
